webpackJsonp([0],{"+R3I":function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"inherit-height",attrs:{id:"data"}},[a("div",{attrs:{id:"home-body"}},[t._m(0),t._v(" "),t._m(1),t._v(" "),a("div",{staticClass:"container"},[a("h2",[t._v("Teachings")]),t._v(" "),a("ul",t._l(t.teachings,function(e){return a("li",{key:e.name},[t._v(t._s(e.number)+" - "+t._s(e.name)+" ("+t._s(e.type)+")")])}))])])])},n=[function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{staticClass:"container row"},[i("div",{staticClass:"col-md-3"},[i("img",{attrs:{id:"profile-picture",src:a("sgov"),alt:"Amit"}})]),t._v(" "),i("div",{staticClass:"col-md-4"},[i("h2",{attrs:{id:"name"}},[t._v("Amit K. Roy-Chowdhury")]),t._v(" "),i("p",[i("strong",[t._v("Professor")]),t._v(", Electrical and Computing Enginering")]),t._v(" "),i("p",[i("strong",[t._v("Cooperating Faculty")]),t._v(", Electrical and Computing Engineering")]),t._v(" "),i("p",[i("strong",[t._v("Winston Chung Hall 431")]),t._v(", UC Riverside")]),t._v(" "),i("div",{staticClass:"row"},[i("div",{staticClass:"col-md-6"},[i("p",[i("strong",[t._v("Phone")]),t._v(": (951) 827-7886")])]),t._v(" "),i("div",{staticClass:"col-md-6"},[i("p",[i("strong",[t._v("Fax")]),t._v(": (951) 827-2425")])]),t._v(" "),i("div",{staticClass:"col-md-6"},[i("p",[i("strong",[t._v("Email")]),t._v(": "),i("a",{attrs:{href:"mailto:amitrc@ee.ucr.edu"}},[t._v("amitrc@ee.ucr.edu")])])]),t._v(" "),i("div",{staticClass:"col-md-12"},[i("p",[i("strong",[t._v("Calendar")]),t._v(": "),i("a",{attrs:{href:"https://exchange.engr.ucr.edu/owa/calendar/d2383983d21c431c88e35b4c56929342@ee.ucr.edu/664d22dea30e4cbe833cc229f2961a444807192837336123402/calendar.html"}},[t._v("Outlook Exchange")])])])])]),t._v(" "),i("div",{staticClass:"col-md-5"},[i("h2",[t._v("Research Interests")]),t._v(" "),i("ul",[i("li",[t._v("Computer Vision and Image Processing")]),t._v(" "),i("li",[t._v("Statistical Signal Processing and Pattern Recognition")]),t._v(" "),i("li",[t._v("Biometrics, especially face and soft biometrics")]),t._v(" "),i("li",[t._v("Vision sensor networks")]),t._v(" "),i("li",[t._v("Machine Learning")]),t._v(" "),i("li",[t._v("Multimedia")]),t._v(" "),i("li",[t._v("Biomedical Image Processing")])])])])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"container row"},[a("div",{staticClass:"col-md-12"},[a("p",[t._v("\n                 Dr. Roy-Chowdhury leads the Video Computing Group at UCR, with research interests in computer vision/image processing, pattern recognition, and statistical signal processing. His group is involved in research projects related to camera networks, human behavior modeling, face recognition, and bioimage analysis. Application domains include national and homeland security, commercial multimedia, home infrastructure, computational biology, and digital arts. The underlying approach of his research is to harness various methods in systems theory, signal processing, machine learning, mathematics and statistics to the analysis of images and videos in order to obtain an understanding of their content. This scientific understanding can lead to machine vision technologies that can provide an automated/semi-automated analysis of the 3D environment from images/videos, analogous to the capabilities of biological visual systems. Prof. Roy-Chowdhury's research has been supported by various agencies including the National Science Foundation, Office of Naval Research, Army Research Office, DARPA, National Endowment for the Humanities, and private industries like CISCO and Lockheed-Martin. His recent book on "),a("a",{attrs:{href:"http://www.morganclaypool.com/doi/abs/10.2200/S00400ED1V01Y201201COV004"}},[t._v("Camera Networks")]),t._v(", the first monograph on the topic, provides an overview of current research in the field. For more details, please see his "),a("a",{attrs:{href:"static/people/amitcv.pdf"}},[t._v("CV")]),t._v(".\n             ")])])])}],s={render:i,staticRenderFns:n};e.a=s},"+VW6":function(t,e,a){"use strict";e.a={data:function(){return{title:"People",facultyMembers:[{name:"Amit Roy-Chowdhury",position:"Professor, Electrical and Computer Engineering",image:"amit.jpg",website:"",contact:{email:"amitrc@ee.ucr.edu",mobile:"951-827-7886",website:"/Amit"}}],alumni:[{name:"Katya Mkrtchyan",position:"Assistant Professor at Cal State Northridge",degree:"PhD 2016",thesis:"Video Bioinformatics with Applications in Cell Biology and Entomology",thesisLink:""},{name:"Mahmudul Hasan",position:"Currently at Comcast Labs",degree:"PhD 2016",thesis:"Online Activity Understanding and Labeling in Natural Videos",thesisLink:"thesis-hasan.pdf"},{name:"Shu Zhang",position:"Currently at JD.com",degree:"PhD 2015",thesis:"Wide-Area Video Understanding: Tracking, Video Summarization and Algorithm-Platform Co-Design",thesisLink:"Shu Thesis.pdf"},{name:"Abir Das",position:"Postdoctoral Researcher, Computer Vision and Learning Group, Boston University",degree:"PhD 2015",thesis:"Active Learning in Multi-Camera Networks, With Applications in Person Re-Identification",thesisLink:"thesis-abir.pdf"},{name:"Ramya Srinivasan",position:"Currently at Fujitsu Lab, USA",degree:"PhD 2015",thesis:"Investigating the Role of Saliency for Face Recognition",thesisLink:"thesis-ramya.pdf"},{name:"Yingying Zhu",position:"Currently at Honda Research Institute",degree:"PhD 2014",thesis:"Towards Sparse Modeling of Multi-Object Interactions in Video",thesisLink:"thesis-yingying.pdf"},{name:"Nandita Nayak",position:"Currently at Konica Minolta Lab",degree:"PhD 2014",thesis:"Graphical Models for Wide-Area Activity Analysis in Continuous Videos",thesisLink:"Nandita_thesis.pdf"},{name:"Anirban Chakraborty",position:"Assistant Professor at Indian Institute of Science",degree:"PhD 2014",thesis:"Exploration of Contextual Relationships for Robust Video Analysis: Applications in Camera Networks, Bio-image Analysis and Activity Forecasting",thesisLink:"thesis-anirban.pdf"},{name:"Chong Ding",position:"Currently at HRL Lab, USA ",degree:"PhD 2013",thesis:"Coordinated Sensing in Intelligent Camera Networks",thesisLink:"thesis-chong.pdf"},{name:"Ahmed Tashrif Kamal",position:"Currently at Apple, USA",degree:"PhD 2013",thesis:"Information Weighted Consensus for Distributed Estimation in Vision Networks",thesisLink:"PhD_Dissertation_Ahmed_Kamal.pdf"},{name:"Min Liu",position:"Associate Prof. at Hunan University, China",degree:"PhD 2012",thesis:"Feature Extraction in Volumetric Bioimages",thesisLink:"thesis-Min.pdf"},{name:"Antony Lam",position:"Asst. Prof at Saitama University, Japan.",degree:"PhD 2010",thesis:"Learning Ranking Functions for Video Search on the Web (co-advised with Prof. C. Shelton)",thesisLink:""},{name:"Ricky Sethi",position:"Asst. Prof. at Fitchburg State University",degree:"PhD 2009",thesis:"A Physics-Based Neurobiologically-Inspired Stochastic Framework for Activity Recognition",thesisLink:"thesis-rick.pdf"},{name:"Bi Song",position:"Currently at Sony Research USA",degree:"PhD 2009",thesis:"Scene Analysis, Control and Communication in Distributed Camera Networks",thesisLink:"Thesis_BiSong.pdf"},{name:"Yilei Xu",position:"Currently at Nokia",degree:"PhD 2008",thesis:"A Theoretical Analysis of Image Appearance Models With Applications in Face Recognition",thesisLink:"Yilei-Final-Thesis.pdf"},{name:"Elliot Staudt",position:"Currently at Mayachitra Inc.",degree:"MS 2015",thesis:"",thesisLink:""},{name:"Utkarsh Gaur",position:"Currently at Bisque Lab, UCSB",degree:"MS 2010",thesis:"",thesisLink:""},{name:"Ting Yeuh Jeng",position:"",degree:"MS 2009",thesis:"",thesisLink:""},{name:"Cristian Soto",position:"",degree:"MS Student 2007-2008",thesis:"",thesisLink:""},{name:"Luis Gonzalez-Argueta",position:"",degree:"MS Student 2007-2009",thesis:"",thesisLink:""}],undergrads:[{name:"Cody Simons",position:"Undergraduate Student, Electrical and Computer Engineering",image:"cody.jpg",website:"",contact:{email:"csimo005@ucr.edu",mobile:"",website:""}}],visitors:[{name:"Niki Martinel",position:"University of Udine, Italy",image:"Niki.jpg",website:"http://web.uniud.it/didattica/servizi_studenti/international_students_service/university.htm",contact:{email:"",mobile:"",website:""}},{name:"Amran Bhuiyan",position:"Istituto Italiano di Tecnologia, Italy",image:"Amran.jpg",website:"",contact:{email:"",mobile:"",website:""}}],grads:[{name:"Jawadul H. Bappy",position:"PhD Candidate, Electrical and Computer Engineering",image:"jawad.png",website:"http://www.ee.ucr.edu/~mbappy/",contact:{email:"mbapp001@ucr.edu",mobile:"951-827-7886",website:"http://www.ee.ucr.edu/~mbappy/"}},{name:"Niluthpol Mithun",position:"PhD Candidate, Electrical and Computer Engineering",image:"Mithun.jpg",website:"",contact:{email:"nmith001@ucr.edu",mobile:"",website:"http://www.ee.ucr.edu/~nmithun/"}},{name:"Rameswar Panda",position:"PhD Candidate, Electrical and Computer Engineering",image:"Panda.png",website:"",contact:{email:"rpand002@ucr.edu",mobile:"",website:"https://rpand002.github.io/"}},{name:"Tahmida Mahmud",position:"PhD Candidate, Electrical and Computer Engineering",image:"tahmida2.jpg",website:"",contact:{email:"tmahm001@ucr.edu",mobile:"",website:"http://www.ece.ucr.edu/~tmahmud/"}},{name:"Sujoy Paul",position:"PhD Candidate, Electrical and Computer Engineering",image:"sujoy.jpg",website:"",contact:{email:"supaul@ece.ucr.edu",mobile:"",website:"http://ee.ucr.edu/~supaul/"}},{name:"Shuyue Lan",position:"PhD Candidate, Electrical and Computer Engineering",image:"shuyue.jpg",website:"",contact:{email:"slan001@ucr.edu",mobile:"",website:"http://www.ee.ucr.edu/~slan/"}},{name:"Sourya Roy",position:"PhD Candidate, Electrical and Compouting Engineering",image:"sourya.jpg",website:"",contact:{email:"souroy099@gmail.com ",mobile:"",website:""}},{name:"Sasha Li",position:"PhD Candidate, Electrical and Computer Engineering",image:"shasha.jpg",website:"",contact:{email:"",mobile:"",website:""}},{name:"Akash Gupta",position:"Masters Student, Electrical and Computer Engineering",image:"Akash.jpg",website:"",contact:{email:"",mobile:"",website:""}}]}}}},"+YSy":function(t,e,a){"use strict";var i=a("hXCR"),n=a("pBMn"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.a=o.exports},"+p9J":function(t,e,a){"use strict";e.a={name:"swiper-slide",data:function(){return{slideClass:"swiper-slide"}},ready:function(){this.update()},mounted:function(){this.update(),this.$parent.options.slideClass&&(this.slideClass=this.$parent.options.slideClass)},updated:function(){this.update()},attached:function(){this.update()},methods:{update:function(){this.$parent&&this.$parent.swiper&&this.$parent.swiper.update&&(this.$parent.swiper.update(!0),this.$parent.options.loop&&this.$parent.swiper.reLoop())}}}},"/Nfc":function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{staticClass:"inherit-height",attrs:{id:"data"}},[i("div",{attrs:{id:"home-body"}},[i("div",{staticClass:"container row"},[i("div",{staticClass:"col-md-12"},[i("h2",{attrs:{id:"name"}},[t._v("Face Tracking and Recognition")]),t._v(" "),i("img",{staticClass:"research-img",attrs:{src:a("lh1m"),alt:"Faces"}}),t._v(" "),i("p",[t._v("\n              Portraits are subject to several complexities such as aesthetic sensibilities of the artist or social standing of the sitter. Moreover, the number of samples available to model these effects is often limited. For robust automated face recognition, it also becomes important to model the characteristics of the artist. From a set of portraiture where the identities of subjects is known, we derive appropriate features that are based on domain knowledge of artistic renderings and learn statistical models for the distributions of the match and non-match scores, which we refer to as the portrait feature space (PFS). The features considered include well-known facial recognition attributes like local features and anthropometric distances. Thereafter, we learn which of the chosen features were emphasized in various works involving (a) same artist depicting same sitter, (b) same sitter but by different artists, (c) same artist but depicting different sitters, and we show that the knowledge of these specific choices can provide valuable information regarding the sitter and/or artist. Further, we use the learned PFS on a number of cases that have been �open questions� to art historians. They are usually in the form of validating two portraits as belonging to the same person. Using statistical hypothesis tests on the PFS, we provide quantitative measures of similarity for each of these questions. It is, to the best of our knowledge, the first study that applies automated face recognition technologies to the analysis of portraits of multiple subjects in various forms - paintings, death masks, sculptures.\n            ")]),t._v(" "),i("hr"),t._v(" "),i("h3",[t._v("Face Tracking and Recognition in Video  "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showFace=!t.showFace}}},[t._v("Toggle")])]),t._v(" "),i("div",{directives:[{name:"show",rawName:"v-show",value:t.showFace,expression:"showFace"}]},[t._m(0),t._v(" "),i("p",[t._v("\n               In this project, we have studied the problem of robust face tracking and recognition in the presence of large pose and illumination changes. First, we developed a theoretical model for understanding the effects of lighting, motion and shape in the process of image formation, bulding upon multilinear tensor algebra. This has led to the development of robust tracking of facial features and an analysis-by-synthesis framework for face recognition from video sequences.\n             ")]),t._v(" "),i("h4",[i("span",{staticClass:"publicationHeader"},[t._v("Sample Publications")]),t._v(" "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showFacePublications=!t.showFacePublications}}},[t._v("Toggle")])]),t._v(" "),i("ul",{directives:[{name:"show",rawName:"v-show",value:t.showFacePublications,expression:"showFacePublications"}],staticClass:"list-group"},t._l(t.facePublications,function(e){return i("li",{key:e.name,staticClass:"list-group-item"},[i("a",{attrs:{href:e.link}},[i("h5",[t._v(t._s(e.name))]),t._v(" "),i("p",[i("small",[t._v(t._s(e.note))])])]),t._v(" "),void 0!==e.extras?i("div",{staticClass:"extraContainer"},t._l(e.extras,function(e){return i("a",{key:e.path,staticClass:"btn btn-primary",attrs:{href:e.path}},[t._v(t._s(e.name))])})):t._e()])}))])])])])])},n=[function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("video",{staticClass:"research-img",attrs:{width:"25%",controls:""}},[i("source",{attrs:{src:a("x+8f"),type:"video/mp4"}})])}],s={render:i,staticRenderFns:n};e.a=s},"0+xk":function(t,e,a){t.exports=a.p+"static/img/HCI_network.fc8e2ac.png"},"08xh":function(t,e){},"0sUC":function(t,e,a){"use strict";var i=a("yVyP"),n=a("VU/8"),s=n(null,i.a,null,null,null);e.a=s.exports},"1OOY":function(t,e,a){t.exports=a.p+"static/img/neh.9f16007.svg"},"1Shb":function(t,e,a){"use strict";var i=a("K7ZM"),n=a("VU/8"),s=n(null,i.a,null,null,null);e.a=s.exports},"1wL/":function(t,e,a){"use strict";var i=a("CGR3"),n=a("dQ+c"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.a=o.exports},"5ZaB":function(t,e,a){t.exports=a.p+"static/img/ucr-logo.7db66ff.png"},"6rSW":function(t,e,a){"use strict";var i=a("gCej"),n=a("VU/8"),s=n(null,i.a,null,null,null);e.a=s.exports},"7A/x":function(t,e,a){t.exports=a.p+"static/img/image4.4638a1c.png"},"7oBO":function(t,e,a){"use strict";var i="undefined"!=typeof window;i&&(window.Swiper=a("gsqX")),e.a={name:"swiper",props:{options:{type:Object,default:function(){return{autoplay:3500}}},notNextTick:{type:Boolean,default:function(){return!1}}},data:function(){return{defaultSwiperClasses:{wrapperClass:"swiper-wrapper"}}},ready:function(){!this.swiper&&i&&(this.swiper=new Swiper(this.$el,this.options))},mounted:function(){var t=this,e=function(){if(!t.swiper&&i){delete t.options.notNextTick;var e=!1;for(var a in t.defaultSwiperClasses)t.defaultSwiperClasses.hasOwnProperty(a)&&t.options[a]&&(e=!0,t.defaultSwiperClasses[a]=t.options[a]);var n=function(){t.swiper=new Swiper(t.$el,t.options)};e?t.$nextTick(n):n()}}(this.options.notNextTick||this.notNextTick)?e():this.$nextTick(e)},updated:function(){this.swiper&&this.swiper.update()},beforeDestroy:function(){this.swiper&&(this.swiper.destroy(),delete this.swiper)}}},"8EWh":function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{attrs:{id:"header-amit"}},[i("router-link",{attrs:{to:"/"}},[i("div",{attrs:{id:"header"}},[i("div",{staticClass:"row inherit-height"},[i("div",{staticClass:"col-md-1 col-xs-2 inherit-height"},[i("img",{staticClass:"img-fluid vc",attrs:{id:"ucr-logo",src:a("5ZaB"),alt:"UCR's Logo"}})]),t._v(" "),i("div",{staticClass:"col-md-11 col-xs-10 inherit-height",attrs:{id:"header-title"}},[i("p",{staticClass:"vc"},[i("span",{staticClass:"primary no-margin h2"},[t._v("Video Computing Group")])])])])])])],1)},n=[],s={render:i,staticRenderFns:n};e.a=s},"8oDi":function(t,e,a){t.exports=a.p+"static/media/Tracking_results.ce4406f.mp4"},"8z7O":function(t,e,a){"use strict";e.a={data:function(){return{showBioPublications:!1,bioPublications:[{name:"Optimal Landmark Selection for Registration of 4D Confocal Image Stacks in Arabidopsis",note:"K. Mkrtchyan, A. Chakraborty, and A. Roy-Chowdhury, IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2016.",link:"static/publications/TCBB.pdf",year:"2016"},{name:"Context Aware Spatio-temporal Cell Tracking In Densely Packed Multilayer Tissues",note:"A. Chakraborty, A. Roy-Chowdhury, Medical Image Analysis, 2014.",link:"static/publications/MedIA-14.pdf",extras:[{name:"Supplemental Material",path:"static/publications/MedIA-Supplementary.pdf"},{name:"Code",path:"static/publications/Context-CRF-Tracker.zip"}],year:"2014"},{name:"Adaptive Geometric Tessellation For 3D Reconstruction of Anisotropically Developing Cells In Multilayer Tissues From Sparse Volumetric Microscopy Images",note:"A. Chakraborty, M. Perales, G. V. Reddy, A. Roy-Chowdhury, PLOS One, 2013.",link:"static/publications/article.html",extras:[],year:"2013"},{name:"Quantitative Analysis of Live-Cell Growth at the Shoot Apex of Arabidopsis thaliana: Algorithms for Feature Measurement and Temporal alignment",note:"M. Tataw, V. Reddy, E. Keogh, A. Roy-Chowdhury, IEEE/ACM Trans. on Computational Biology and Biomedicine, 2013 (In Press).",link:"static/publications/tataw_tcbb_final.pdf",extras:[],year:"2013"},{name:"Automated Registration of Live Imaging Stacks of Arabidopsis",note:"K. Mkrtchyan, A. Chakraborty, A. Roy-Chowdhury, International Symposium on Biomedical Imaging, 2013.",link:"static/publications/isbi2013_Katya.pdf",extras:[],year:"2013"},{name:"Cell Resolution 3D Reconstruction of Developing Multilayer Tissues from Sparsely Sampled Volumetric Microscopy Images",note:"A. Chakraborty, R. Yadav, G. V. Reddy, A. Roy-Chowdhury, IEEE Intl. Conf. on Bioinformatics and Biomedicine, 2011.",link:"static/publications/bibm11-anirban.pdf",extras:[],year:"2011"},{name:"Adaptive Cell Segmentation and Tracking for Volumetric Confocal Microscopy Images of A Developing Plant Meristem",note:"M. Liu, A. Chakraborty, D. Singh, M. Gopi, R. Yadav, G.V. Reddy, and A. Roy-Chowdhury, Molecular Plant, 2011.",link:"static/publications/tracking-molplant.pdf",extras:[],year:"2011"}]}}}},BMCm:function(t,e){},BNYb:function(t,e,a){"use strict";function i(t){a("08xh")}var n=a("FEEd"),s=a("fVvD"),o=a("VU/8"),r=i,l=o(n.a,s.a,r,null,null);e.a=l.exports},CGR3:function(t,e,a){"use strict";e.a={data:function(){return{showVideoPublications:!1,videoPublications:[{name:"Model-based Multi-view Video Compression Using Distributed Source Coding Principles",note:"J. Nayak, B. Song, E. Tuncel, A. Roy-Chowdhury. Distributed Source Coding: Theory, Algorithms and Applications (Eds. P. Luigi and M. Gatspar), Elsevier, 2009.",link:"static/publications/MultiViewDVC.pdf",extras:[],year:"2009"},{name:"A Multi-Terminal Model-Based Video Compression Algorithm",note:"B. Song, A. Roy-Chowdhury, E. Tuncel, IEEE Intl. Conf. on Image Processing, 2006.",link:"static/publications/icip06-distributed.pdf",extras:[],year:"2006"},{name:"Towards A Multi-Terminal Video Compression Algorithm Using Epipolar Geometry",note:"B. Song, O. Bursalioglu, E. Tuncel, A. Roy-Chowdhury, IEEE Intl. Conf. on Acoustics, Speech and Signal Processing, 2006. (Best Student Paper Award)",link:"static/publications/icassp06.pdf",extras:[],year:"2006"}]}}}},DIZM:function(t,e,a){t.exports=a.p+"static/img/Graphical_Abstract.795b7dc.png"},Dd1x:function(t,e,a){t.exports=a.p+"static/img/google.4414115.png"},DiwX:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"inherit-height",attrs:{id:"data"}},[a("div",{attrs:{id:"home-body"}},[a("div",{staticClass:"container row"},[a("div",{staticClass:"col-md-12"},[t._m(0),t._v(" "),a("div",{staticClass:"alert alert-danger",attrs:{role:"alert"}},[t._v("The contributing authors include the documents contained in these directories as a means to ensure timely dissemination of scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright. These works may not be reposted without the explicit permission of the copyright holder.")]),t._v(" "),t._m(1),t._v(" "),a("div",{staticClass:"panel panel-primary"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("Books and Edited Books "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showBooks=!t.showBooks}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showBooks,expression:"showBooks"}],staticClass:"list-group"},t._l(t.books,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("a",{attrs:{href:e.link}},[a("h3",[t._v(t._s(e.name))]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])])])])}))]),t._v(" "),a("h2",[t._v("Publications")]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2017 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2017=!t.showPublications2017}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2017,expression:"showPublications2017"}],staticClass:"list-group"},t._l(t.articles2017,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2016 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2016=!t.showPublications2016}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2016,expression:"showPublications2016"}],staticClass:"list-group"},t._l(t.articles2016,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2015 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2015=!t.showPublications2015}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2015,expression:"showPublications2015"}],staticClass:"list-group"},t._l(t.articles2015,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2014 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2014=!t.showPublications2014}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2014,expression:"showPublications2014"}],staticClass:"list-group"},t._l(t.articles2014,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2013 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2013=!t.showPublications2013}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2013,expression:"showPublications2013"}],staticClass:"list-group"},t._l(t.articles2013,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2012 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2012=!t.showPublications2012}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2012,expression:"showPublications2012"}],staticClass:"list-group"},t._l(t.articles2012,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2011 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2011=!t.showPublications2011}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2011,expression:"showPublications2011"}],staticClass:"list-group"},t._l(t.articles2011,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2010 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2010=!t.showPublications2010}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2010,expression:"showPublications2010"}],staticClass:"list-group"},t._l(t.articles2010,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2009 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2009=!t.showPublications2009}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2009,expression:"showPublications2009"}],staticClass:"list-group"},t._l(t.articles2009,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2008 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2008=!t.showPublications2008}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2008,expression:"showPublications2008"}],staticClass:"list-group"},t._l(t.articles2008,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2007 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2007=!t.showPublications2007}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2007,expression:"showPublications2007"}],staticClass:"list-group"},t._l(t.articles2007,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2006 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2006=!t.showPublications2006}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2006,expression:"showPublications2006"}],staticClass:"list-group"},t._l(t.articles2006,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2005 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2005=!t.showPublications2005}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2005,expression:"showPublications2005"}],staticClass:"list-group"},t._l(t.articles2005,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2004 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2004=!t.showPublications2004}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2004,expression:"showPublications2004"}],staticClass:"list-group"},t._l(t.articles2004,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))]),t._v(" "),a("div",{staticClass:"panel panel-info"},[a("div",{staticClass:"panel-heading"},[a("h3",{staticClass:"panel-title"},[t._v("2003 "),a("span",{staticClass:"fa fa-bars toggle",on:{click:function(e){t.showPublications2003=!t.showPublications2003}}})])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showPublications2003,expression:"showPublications2003"}],staticClass:"list-group"},t._l(t.articles2003,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("h3",[a("a",{attrs:{href:e.link}},[t._v(t._s(e.name))]),t._v(" "),a("small",[void 0!==e.extras?a("span",t._l(e.extras,function(e){return a("a",{key:e.path,attrs:{href:e.path}},[t._v("["+t._s(e.name)+"]")])})):t._e()])]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"}):t._e()])}))])])])])])},n=[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("p",[a("small")])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"alert alert-info"},[t._v("\n            A complete list of articles by publication category is available "),a("a",{attrs:{href:"../../static/publications/publications.pdf"}},[a("strong",[t._v("here")])]),t._v(".\n          ")])}],s={render:i,staticRenderFns:n};e.a=s},E5uT:function(t,e,a){"use strict";e.a={name:"publications",data:function(){return{showPublications2017:!1,showPublications2016:!1,showPublications2015:!1,showPublications2014:!1,showPublications2013:!1,showPublications2012:!1,showPublications2011:!1,showPublications2010:!1,showPublications2009:!1,showPublications2008:!1,showPublications2007:!1,showPublications2006:!1,showPublications2005:!1,showPublications2004:!1,showPublications2003:!1,showBooks:!1,showMagazines:!1,showThesis:!1,books:[{name:"Camera Networks: The Acquisition and Analysis of Videos Over Wide Areas",note:"A. Roy-Chowdhury and B. Song. Morgan and Claypool (Synthesis Lectures in Computer Vision), 2012.",link:"http://www.morganclaypool.com/doi/abs/10.2200/S00400ED1V01Y201201COV004"},{name:"Distributed Video Sensor Networks",note:"Eds. B. Bhanu, C. Ravishankar, A. Roy-Chowdhury, H. Aghajan, D. Terzopoulos. Springer, 2010.",link:"http://www.springer.com/computer/image+processing/book/978-0-85729-126-4"},{name:"Recognition of Humans and Their Activities Using Video",note:"R. Chellappa, A. Roy-Chowdhury, S. Zhou. Morgan and Claypool (Synthesis Lectures in Image, Video, & Multimedia Processing), 2005.",link:"http://www.amazon.com/gp/product/1598290061/qid=1134776125/sr=1-1/ref=sr_1_1/104-3732392-4137503?s=books&v=glance&n=283155"}],articles2017:[{name:"Joint Prediction of Activity Labels and Starting Times in Untrimmed Videos",note:"T. Mahmud, M. Hasan and A. Roy-Chowdhury, International Conference on Computer Vision, 2017.",link:"static/publications/ICCV_Tahmida.pdf",year:"2017"},{name:"Exploiting Spatial Structure for Localizing Manipulated Image Regions",note:"J. H. Bappy, A. Roy-Chowdhury, J. Bunk, L. Nataraj and B. S. Manjunath, International Conference on Computer Vision, 2017.",link:"static/publications/ICCV_Jawad.pdf",year:"2017"},{name:"Weakly Supervised Summarization of Web Videos",note:"R. Panda, A. Das, Z. Wu, J. Ernst and A. Roy-Chowdhury, International Conference on Computer Vision, 2017.",link:"static/publications/ICCV_Jawad.pdf",year:"2017"},{name:"Diversity-aware Multi-Video Summarization",note:"R. Panda, N. C. Mithun and A. Roy-Chowdhury, IEEE Trans. on Image Processing, 2017.",link:"static/publications/TIP_Rameswar.pdf",extras:[{name:"Supplemental Material",path:"static/publications/TIP_Supp_Rameswar.pdf"}],year:"2017"},{name:"Multi-View Surveillance Video Summarization via Joint Embedding and Sparse Optimization",note:"R. Panda and A. Roy-Chowdhury, IEEE Trans. on Multimedia, 2017.",link:"static/publications/TMM_Rameswar.pdf",year:"2017"},{name:"Detection and Localization of Image Forgeries using Resampling Features and Deep Learning",note:"J. Bunk, J. H. Bappy, T. M. Mohammed, L. Nataraj, A. Flenner, B.S. Manjunath, S. Chandrasekaran, A. Roy-Chowdhury and L. Peterson, IEEE Conf. on Computer Vision and Pattern Recognition Workshop, 2017.",link:"static/publications/CVPR_Workshop_Jawad.pdf",year:"2017"},{name:"Unsupervised Adaptive Re-identification in Open World Dynamic Camera Networks",note:"R. Panda, A. H. Bhuiyan, V. Murino and A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2017 (Spotlight).",link:"static/publications/cvpr2017reid.pdf",year:"2017"},{name:"Collaborative Summarization of Topic-Related Videos",note:"R. Panda and A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2017.",link:"static/publications/cvpr2017summ.pdf",year:"2017"},{name:"Non-Uniform Subset Selection for Active Learning in Structured Data",note:"S. Paul, J. H. Bappy, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2017.",link:"static/publications/cvpr2017subset.pdf",year:"2017"},{name:"The Impact of Typicality for Informative Representative Selection",note:"J. H. Bappy, S. Paul, E. Tuncel and A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2017.",link:"static/publications/cvpr2017typicality.pdf",year:"2017"}],articles2016:[{name:"Continuous adaptation of multi-camera person identification models through sparse non-redundant representative selection",note:"A. Das, R. Panda, A. Roy-Chowdhury, Computer Vision and Image Understanding, 2016.",link:"static/publications/CVIU_2016_Abir.pdf",extras:[{name:"Supplemental Material",path:"static/publications/Supplementary_CVIU_2016_Abir.pdf"}],year:"2016"},{name:"Distributed Multi-target Tracking and Data Association in Vision Networks",note:"A. T. Kamal, J. H. Bappy, J. A. Farrell, A. Roy-Chowdhury, IEEE Trans. on Pattern Analysis and Machine Intelligence, 2016.",link:"static/publications/MTIC-TPAMI.pdf",extras:[{name:"Code",path:"static/publications/MTIC_Matlab_Code.zip"}],year:"2016"},{name:"Network Consistent Data Association",note:"A. Chakraborty, A. Das, A. Roy-Chowdhury, IEEE Trans. on Pattern Analysis and Machine Intelligence, 2016.",link:"static/publications/Camera_Ready_Manuscript.pdf",extras:[{name:"Supplemental Material",path:"static/publications/supp_NCDA.pdf"},{name:"Code",path:"static/publications/NCDA_Code.zip"}],year:"2016"},{name:"Online Adaptation for Joint Scene and Object Classification",note:"J. H. Bappy, S. Paul, A. Roy-Chowdhury, European Conf. on Computer Vision, 2016.",link:"static/publications/eccv2016_jawad.pdf",year:"2016"},{name:"Temporal Model Adaptation for Person Re-Identification",note:"N. Martinel, C. Micheloni, A. Roy-Chowdhury, European Conf. on Computer Vision, 2016.",link:"static/publications/niki-eccv16.pdf",year:"2016"},{name:"Context-Aware Video Summarization",note:"S. Zhang, Y. Zhu, A. Roy-Chowdhury, IEEE Trans. on Image Processing, 2016.",link:"static/publications/TIP2016_summarization.pdf",year:"2016"},{name:"Learning Temporal Regularity in Video Sequences",note:"M. Hasan, J. Choi, J. Neumann, A. Roy-Chowdhury, and L. Davis, IEEE Conf. on Computer Vision and Pattern Recognition, 2016.",link:"static/publications/cvpr2016_regularity.pdf",extras:[{name:"Code",path:"static/publications/regularity.html"}],year:"2016"},{name:"Opportunistic Image Acquisition of Individual and Group Activities in a Distributed Camera Network",note:"C. Ding, J. H. Bappy, J. A. Farrell, A. Roy-Chowdhury, IEEE Transactions on Circuits and Systems for Video Technology, 2016.",link:"static/publications/TCSVT_2016.pdf",year:"2016"},{name:"Generating Diverse Image Datasets with Limited Labeling",note:"N. C. Mithun, R. Panda, A. Roy-Chowdhury, ACM International Conf. on Multimedia, 2016.",link:"static/publications/ACM_2016.pdf",year:"2016"},{name:"Incremental Learning of Human Activity Models from Videos",note:"M. Hasan, and A. Roy-Chowdhury, Computer Vision and Image Understanding, 2016.",link:"static/publications/2016_cviu.pdf",extras:[{name:"Code",path:"static/publications/incremental.html"}],year:"2016"},{name:"Optimal Landmark Selection for Registration of 4D Confocal Image Stacks in Arabidopsis",note:"K. Mkrtchyan, A. Chakraborty, and A. Roy-Chowdhury, IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2016.",link:"static/publications/TCBB.pdf",year:"2016"},{name:"Video Summarization in a Multi-View Camera Network",note:"R. Panda, A. Das, A. Roy-Chowdhury, International Conf. on Pattern Recognition, 2016.",link:"static/publications/ICPR_Rameswar.pdf",year:"2016"},{name:"Inter-dependent CNNs for Joint Scene and Object Recognition",note:"J. H. Bappy, A. Roy-Chowdhury, International Conf. on Pattern Recognition, 2016.",link:"static/publications/ICPR2016_SO_Jawad.pdf",year:"2016"},{name:"A Poisson Process Model for Activity Forecasting",note:"T. Mahmud, M. Hasan, A. Chakraborty, A. Roy-Chowdhury, IEEE International Conf. on Image Processing, 2016.",link:"static/publications/icip2016_timeforecast.pdf",year:"2016"},{name:"Efficient Selection of Informative and Diverse Training Samples with Applications in Scene Classification",note:"S. Paul, J. H. Bappy, A. Roy-Chowdhury, IEEE International Conf. on Image Processing, 2016.",link:"static/publications/icip2016_trainingsampleselection.pdf",year:"2016"},{name:"Adaptive Algorithm Selection, with Applications in Pedestrian Detection",note:"S. Zhang, Q. Zhu, A. Roy-Chowdhury, IEEE International Conf. on Image Processing, 2016.",link:"static/publications/icip2016_pedestrian.pdf",year:"2016"}],articles2015:[{name:"Context Aware Active Learning of Activity Recognition Models",note:"M. Hasan, A. Roy-Chowdhury, International Conference on Computer Vision, 2015.",link:"static/publications/ICCV2015.pdf",extras:[{name:"Code",path:"static/publications/caal.html"}],year:"2015"},{name:"A Continuous Learning Framework for Activity Recognition Using Deep Hybrid Feature Models",note:"M. Hasan, A. Roy-Chowdhury, IEEE Trans. on Multimedia, 2015.",link:"static/publications/tmm2015.pdf",extras:[{name:"Code",path:"static/publications/hybrid.html"}],year:"2015"},{name:"Re-Identification in the Function Space of Feature Warps",note:"A. Das, N. Martinel, C. Micheloni, A. Roy-Chowdhury, IEEE Trans. on Pattern Analysis and Machine Intelligence, 2015.",link:"static/publications/PAMI14-Reid.pdf",extras:[{name:"Supplemental Material",path:"static/publications/Supplementary-PAMI2014-Reid.pdf"}],year:"2015"},{name:"Context-Aware Activity Modeling using Hierarchical Conditional Random Fields",note:"Y. Zhu, N. Nayak, A. Roy-Chowdhury, IEEE Trans. on Pattern Analysis and Machine Intelligence, 2015.",link:"static/publications/PAMI14-Activity.pdf",extras:[{name:"Code",path:"static/publications/download.html"}],year:"2015"},{name:"Computerized Face Recognition in Renaissance Portrait Art",note:"R. Srinivasan, C. Rudolph, and A. K. Roy-Chowdhury, Signal Processing Magazine. 2015.",link:"static/publications/spm2015.pdf",extras:[{name:"Supplmental Material",path:"static/publications/SPM2015_SM.pdf"}],year:"2015"},{name:"Tracking multiple interacting targets in a camera network",note:"S. Zhang, Y. Zhu, and A. K. Roy-Chowdhury, Computer Vision and Image Understanding special issue on Image Understanding for Real-world Distributed Video Networks. 2015.",link:"static/publications/Shu CVIU final.pdf",year:"2015"},{name:"Hierarchical Graphical Models for Simultaneous Tracking and Recognition in Wide-Area Scenes",note:"N. M. Nayak, Y. Zhu, and A. K. Roy-Chowdhury, IEEE Transactions on Image Processing, 2015.",link:"static/publications/tip_twocolumn_revised.pdf",year:"2015"},{name:"A Camera Network Tracking (CamNet) Dataset and Performance Baseline",note:"S. Zhang, E. Staudt, T. Faltemier, A. Roy-Chowdhury, IEEE Winter Conference on Applications of Computer Vision, 2015.",link:"egpaper_final.pdf",extras:[{name:"CamNeT Dataset",path:"static/publications/0B7uDdIqGrZlVfmdlOFZyUEg3RHUxaUdSaGVGTTNDT3R0dUNLSFdCSkZYVWk0dE16TFg4cTA.html"}],year:"2015"}],articles2014:[{name:"Context Aware Spatio-temporal Cell Tracking In Densely Packed Multilayer Tissues",note:"A. Chakraborty, A. Roy-Chowdhury, Medical Image Analysis, 2014.",link:"static/publications/MedIA-14.pdf",extras:[{name:"Supplemental Material",path:"static/publications/MedIA-Supplementary.pdf"},{name:"Code",path:"static/publications/Context-CRF-Tracker.zip"}],year:"2014"},{name:"Context-Aware Activity Forecasting",note:"A. Chakraborty, A. Roy-Chowdhury, Asian Conf. on Computer Vision, 2014.",link:"static/publications/ACCV_2014.pdf",extras:[],year:"2014"},{name:"Consistent Re-identification In A Camera Network",note:"A. Das, A. Chakraborty, A. Roy-Chowdhury, European Conf. on Computer Vision, 2014.",link:"static/publications/eccv2014-2.pdf",extras:[{name:"Supplemental Material",path:"static/publications/NCR_ECCV2014_Supplementary.pdf"},{name:"Code",path:"static/publications/NCR_Code.html"}],year:"2014"},{name:"Continuous Learning of Human Activity Models Using Deep Nets",note:"M. Hasan, A. Roy-Chowdhury, European Conf. on Computer Vision, 2014.",link:"static/publications/eccv2014-1.pdf",extras:[{name:"Code",path:"static/publications/hybrid (1).html"}],year:"2014"},{name:"Incremental Activity Modeling and Recognition in Streaming Videos",note:"M. Hasan, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2014.",link:"static/publications/cvpr2014.pdf",extras:[],year:"2014"},{name:"Learning A Sparse Dictionary of Video Structure for Activity Modeling",note:"N. Nayak, A. Roy-Chowdhury, IEEE Intl. Conf. on Image Processing, 2014.",link:"static/publications/icip2014.pdf",extras:[],year:"2014"},{name:"Distributed Constrained Optimization for Bayesian Opportunistic Visual Sensing",note:"A. Morye, C. Ding, J. A. Farrell, A. Roy-Chowdhury, IEEE Trans. on Control Systems Technology, 2014.",link:"static/publications/akshay_tcst.pdf",extras:[],year:"2014"},{name:"Managing Redundant Content in Bandwidth Constrained Wireless Networks",note:"T. Dao, S. Krishnamurthy, A. Roy-Chowdhury, T. LaPorta, International Conf. on emerging Networking EXperiments and Technologies, 2014.",link:"static/publications/conext_2014.pdf",extras:[],year:"2014"}],articles2013:[{name:"Exploiting Spatio-Temporal Scene Structure for Wide-Area Activity Analysis in Unconstrained Environments",note:"N. Nayak, Y. Zhu, A. Roy-Chowdhury, IEEE Trans. on Information Forensics and Security (Special Issue on Intelligent Video Surveillance), 2013.",link:"static/publications/tifs_context.pdf",extras:[],year:"2013"},{name:"Information Weighted Consensus Filters and their Application in Distributed Camera Networks",note:"A. Kamal, J. A. Farrell, A. Roy-Chowdhury, IEEE Trans. on Automatic Control, 2013.",link:"static/publications/ICF_TAC.pdf",extras:[{name:"Code",path:"static/publications/ICF_Matlab_Code.zip"}],year:"2013"},{name:"Context-Aware Modeling and Recognition of Activities in Video",note:"Y. Zhu, N. Nayak, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2013 (Oral).",link:"static/publications/cvpr2013_context.pdf",extras:[{name:"Code",path:"static/publications/download (1).html"}],year:"2013"},{name:"Recognizing the Royals - Leveraging Computerized Face Recognition for Identifying Subjects in Ancient Artworks",note:"R. Srinivasan, A. Roy-Chowdhury, C. Rudolph, J. Kohl, ACM Intl. Conf. on Multimedia, 2013.",link:"static/publications/acm-mm2013.pdf",extras:[],year:"2013"},{name:"Information Consensus for Distributed Multi-Target Tracking",note:"A. Kamal, J. A. Farrell, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2013.",link:"static/publications/CVPR2013_MTIC.pdf",extras:[{name:"Supplemental Material",path:"static/publication/CVPR2013_MTIC_Supplementary.pdf"},{name:"Code",path:"static/publication/MTIC_Matlab_Code (1).zip"}],year:"2013"},{name:'Modeling Multi-object Interactions using "String of Feature Graphs"',note:"Y. Zhu, N. Nayak, U. Gaur, B. Song, A. Roy-Chowdhury, Computer Vision and Image Understanding, 2013.",link:"static/publications/cviu13.pdf",extras:[],year:"2013"},{name:"Adaptive Geometric Tessellation For 3D Reconstruction of Anisotropically Developing Cells In Multilayer Tissues From Sparse Volumetric Microscopy Images",note:"A. Chakraborty, M. Perales, G. V. Reddy, A. Roy-Chowdhury, PLOS One, 2013.",link:"static/publications/article.html",extras:[],year:"2013"},{name:"Context-Aware Activity Recognition and Anomaly Detection in Video",note:"Y. Zhu, N. Nayak, A. Roy-Chowdhury, IEEE Journal on Selected Topics in Signal Processing, Special Issue on Anomalous Pattern Discovery, February 2013.",link:"static/publications/jstsp13.pdf",extras:[{name:"Code",path:"static/publications/download (2).html"}],year:"2013"},{name:"Vector Field Analysis for Multi-Object Behavior Modeling",note:"N. Nayak, Y. Zhu, A. Roy-Chowdhury, Image and Vision Computing, 2013.",link:"static/publications/N. Nayak, Y. Zhu, A. Roy-Chowdhury, Image and Vision Computing, 2013.",extras:[],year:"2013"},{name:"Quantitative Analysis of Live-Cell Growth at the Shoot Apex of Arabidopsis thaliana: Algorithms for Feature Measurement and Temporal alignment",note:"M. Tataw, V. Reddy, E. Keogh, A. Roy-Chowdhury, IEEE/ACM Trans. on Computational Biology and Biomedicine, 2013 (In Press).",link:"static/publications/tataw_tcbb_final.pdf",extras:[],year:"2013"},{name:"Automated Registration of Live Imaging Stacks of Arabidopsis",note:"K. Mkrtchyan, A. Chakraborty, A. Roy-Chowdhury, International Symposium on Biomedical Imaging, 2013.",link:"static/publications/isbi2013_Katya.pdf",extras:[],year:"2013"}],articles2012:[{name:"Collaborative Sensing In A Distributed PTZ Camera Network",note:"C. Ding, B. Song, A. Morye, J. A. Farrell, A. Roy-Chowdhury, IEEE Trans. on Image Processing, 2012.",link:"static/publications/tip_cding2012.pdf",extras:[],year:"2012"},{name:"Features with Feeling - Incorporating User Preferences in Video Categorization",note:"R. Srinivasan, A. Roy-Chowdhury, Asian Conference on Computer Vision, 2012.",link:"static/publications/accv-2012-final.pdf",extras:[],year:"2012"},{name:"Information Weighted Consensus",note:"A. Kamal, J. A. Farrell, A. Roy-Chowdhury, IEEE Controls and Decision Conf., 2012.",link:"static/publications/CDC_2012.pdf",extras:[{name:"Code",path:"static/publications/CameraNetworks.php"}],year:"2012"},{name:"Alignment of Real-Time Live-Cell Growth Data for Quantitative Analysis of Growth at the Shoot Apex of Arabidopsis thaliana",note:"M. Tataw, V. Reddy, A. Roy-Chowdhury, ACM Conference on Bioinformatics, Computational Biology and Biomedicine, 2012.",link:"static/publications/publications.php",extras:[],year:"2012"},{name:"Opportunistic Sensing In A Distributed PTZ Camera Network",note:"C. Ding, A. Morye, J. A. Farrell, A. Roy-Chowdhury, IEEE Intl. Conf. on Distributed Smart Cameras, 2012.",link:"static/publications/icdsc2012.pdf",extras:[],year:"2012"},{name:"Coordinated Sensing and Tracking for Mobile Camera Platforms",note:"C. Ding, A. Morye, J. A. Farrell, A. Roy-Chowdhury, American Controls Conf., 2012.",link:"static/publications/ACC_2012.pdf",extras:[],year:"2012"}],articles2011:[{name:"Integrated Sensing and Analysis for Wide Area Scene Understanding",note:"B. Song, C. Ding, A. Kamal, J. Farrell, A. Roy-Chowdhury, Signal Processing Magazine, May 2011.",link:"static/publications/SPM_camnetwork.pdf",extras:[],year:"2011"},{name:"A Physics-Based Analysis of Image Appearance Models",note:"Y. Xu, A. Roy-Chowdhury, IEEE Trans. on Pattern Analysis and Machine Intelligence, August 2011.",link:"static/publications/PAMI_2011.pdf",extras:[{name:"Supplemental Material",path:"static/publication/SupplementaryMaterial_PAMI2011.pdf"}],year:"2011"},{name:'A "String of Feature Graphs" Model for Recognition of Complex Activities in Natural Videos',note:"U. Gaur, Y. Zhu, B. Song, A. Roy-Chowdhury, IEEE Conf. on Computer Vision, 2011.",link:"static/publications/iccv-SFG.pdf",extras:[],year:"2011"},{name:"Cell Resolution 3D Reconstruction of Developing Multilayer Tissues from Sparsely Sampled Volumetric Microscopy Images",note:"A. Chakraborty, R. Yadav, G. V. Reddy, A. Roy-Chowdhury, IEEE Intl. Conf. on Bioinformatics and Biomedicine, 2011.",link:"static/publications/bibm11-anirban.pdf",extras:[],year:"2011"},{name:"A Large-scale Benchmark Dataset for Event Recognition in Surveillance Video",note:"Sangmin Oh, Anthony Hoogs, Amitha Perera, Naresh Cuntoor, C.-C. Chen, Jong Taek Lee, Saurajit Mukherjee, J. K. Aggarwal, Hyungtae Lee, Larry Davis, Eran Swears, Xioyang Wang, Qiang Ji, Kishore Reddy, Mubarak Shah, Carl Vondrick, Hamed Pirsiavash, Deva Ramanan, Jenny Yuen, Antonio Torralba, Bi Song, Anesco Fong, Amit Roy-Chowdhury, and Mita Desai, IEEE Conf. on Computer Vision and Pattern Recognition, 2011.",link:"static/publications/cvpr2011.pdf",extras:[],year:"2011"},{name:"Adaptive Cell Segmentation and Tracking for Volumetric Confocal Microscopy Images of A Developing Plant Meristem",note:"M. Liu, A. Chakraborty, D. Singh, M. Gopi, R. Yadav, G.V. Reddy, and A. Roy-Chowdhury, Molecular Plant, 2011.",link:"static/publications/tracking-molplant.pdf",extras:[],year:"2011"},{name:"Motion Pattern Analysis for Modeling and Recognition of Complex Human Activities",note:"N. Nayak, R. Sethi, B. Song, A. Roy-Chowdhury, in Guide to Video Analysis of Humans: Looking at People (Eds., T. Moeslund, A. Hilton, V. Kruger, L. Sigal), Springer 2011.",link:"static/publications/Activity_chapter_nandita.pdf",extras:[],year:"2011"},{name:"Robust Wide Area Tracking in Single and Multiple Views",note:"B. Song, R. Sethi, A. Roy-Chowdhury, in Guide to Video Analysis of Humans: Looking at People (Eds., T. Moeslund, A. Hilton, V. Kruger, L. Sigal), Springer 2011",link:"static/publications/LAPbook_tracking.pdf",extras:[],year:"2011"},{name:"A Generalized Kalman Consensus Filter for Wide Area Video Networks",note:"A. Kamal, C. Ding, B. Song, J. A. Farrell, A. Roy-Chowdhury, Controls and Decision Conference, 2011.",link:"static/publications/cdc11.pdf",extras:[],year:"2011"}],articles2010:[{name:"Tracking and Activity Recognition Through Consensus in Distributed Camera Networks",note:"B. Song, A. Kamal, C. Soto, C. Ding, J. Farrell, A. Roy-Chowdhury, IEEE Trans. on Image Processing, 2010.",link:"static/publications/TIP_consensus2010.pdf",extras:[],year:"2010"},{name:"Automated tracking of stem cell lineages of Arabidopsis shoot apex using local graph matching",note:"M. Liu, R. Yadav, A. Roy-Chowdhury, G.V. Reddy, The Plant Journal, 2010.",link:"static/publications/tracking-plantjournal.pdf",extras:[{name:"Supplemental Material",path:"static/publications/Supplementary Data_PlantJournal.pdf"}],year:"2010"},{name:"A Stochastic Graph Evolution Framework for Robust Multi-Target Tracking,",note:"B. Song, T. Jeng, E. Staudt, A. Roy-Chowdhury, European Conference on Computer Vision, 2010.",link:"static/publications/eccv2010.pdf",extras:[],year:"2010"},{name:"Multilinear Feature Extraction and Classification of Multi-Focal Images, With Applications in Nematode Taxonomy",note:"M. Liu, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2010.",link:"static/publications/cvpr2010.pdf",extras:[],year:"2010"},{name:"Interactive Event Search Using Transfer Learning",note:"A. Lam, A. Roy-Chowdhury, C. Shelton, Asian Conf. on Computer Vision, 2010.",link:"static/publications/accv10.pdf",extras:[],year:"2010"},{name:"The Human Action Image",note:"R.J. Sethi, A. Roy-Chowdhury, Intl. Conf. on Pattern Recognition, 2010.",link:"static/publications/icpr2010_hai.pdf",extras:[],year:"2010"},{name:"A Neurobiologically Motivated Stochastic Method for Analysis of Human Activities in Video",note:"R. Sethi, A. Roy-Chowdhury, Intl. Conf. on Pattern Recognition, 2010.",link:"static/publications/icpr2010_ddhmc.pdf",extras:[],year:"2010"}],articles2009:[{name:"Rate-invariant Recognition of Humans and Their Activities",note:"A. Veeraraghavan, A. Srivastava, A. Roy-Chowdhury and R. Chellappa, IEEE Trans. on Image Processing, June 2009.",link:"static/publications/Rate-invariant.pdf",extras:[],year:"2009"},{name:"Distributed Multi-Target Tracking In A Self-Configuring Camera Network",note:"C. Soto, B. Song, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2009.",link:"static/publications/cvpr09.pdf",extras:[],year:"2009"},{name:"Face Tracking",note:"A. Roy-Chowdhury and Y. Xu, Encyclopedia of Biometrics, Springer, 2009.",link:"static/publications/encycl_biometrics.pdf",extras:[],year:"2009"},{name:"Model-based Multi-view Video Compression Using Distributed Source Coding Principles",note:"J. Nayak, B. Song, E. Tuncel, A. Roy-Chowdhury. Distributed Source Coding: Theory, Algorithms and Applications (Eds. P. Luigi and M. Gatspar), Elsevier, 2009.",link:"static/publications/MultiViewDVC.pdf",extras:[],year:"2009"},{name:"Combining Geometrical and Statistical Models for Video-based Face Recognition",note:"A. Roy-Chowdhury and Y. Xu., Multi-Biometric Systems for Identity Recognition: Theory and Experiments, (Eds. N.V. Boulgouris, K.N. Plataniotis and E. Micheli-Tzanakou), IEEE Press, In Press.",link:"static/publications/GAM BookChapter.pdf",extras:[],year:"2009"},{name:"Query-based Retrieval of Complex Activities using �Strings of Motion-Words�",note:"U. Gaur, B. Song, A. Roy-Chowdhury, IEEE Workshop on Motion and Video Computing, 2009.",link:"static/publications/wmvc09-2.pdf",extras:[],year:"2009"}],articles2008:[{name:"Robust Tracking in A Camera Network: A Multi-Objective Optimization Framework",note:"B. Song and A. Roy-Chowdhury, IEEE Journal on Selected Topics in Signal Processing: Special Issue on Distributed Processing in Vision Networks, August 2008.",link:"static/publications/stamp.html",extras:[],year:"2008"},{name:"Inverse Compositional Estimation of 3D Pose And Lighting in Dynamic Scenes",note:"Y. Xu and A. Roy-Chowdhury, IEEE Trans. on Pattern Analysis and Machine Intelligence, July 2008.",link:"static/publications/pami08.pdf",extras:[],year:"2008"},{name:"A Theoretical Analysis of Linear and Multi-linear Models of Image Appearance",note:"Y. Xu, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2008.",link:"static/publications/cvpr08_theory.pdf",extras:[],year:"2008"},{name:"Learning A Geometry-Integrated Image Appearance Manifold From A Small Training Set",note:"Y. Xu, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2008.",link:"static/publications/cvpr08_GAM.pdf",extras:[],year:"2008"},{name:"Integrating Illumination, Motion and Shape Models for Robust Face Recognition In Video",note:"Y.Xu, A. Roy-Chowdhury and K. Patel, EURASIP Journal on Advances in Signal Processing: Advanced Signal Processing and Pattern Recognition Methods for Biometrics, 2008.",link:"static/publications/download (3).html",extras:[],year:"2008"},{name:"Activity Representation Using 3D Shape Models",note:"M. Abdelkader, A. Roy-Chowdhury, R. Chellappa and U. Akdemir, EURASIP Journal on Image and Video Processing: Special Issue on Anthropocentric Video Analysis: Tools and Applications, 2008.",link:"static/publications/paper_Eurasip.pdf",extras:[],year:"2008"},{name:"Multi-Target Tracking Through Opportunistic Camera Control In A Resource Constrained Multimodal Sensor Network",note:"J. Nayak, L. Gonzalez-Argueta, B. Song, A. Roy-Chowdhury, and E. Tuncel, IEEE/ACM Intl. Conf. on Distributed Smart Cameras, 2008.",link:"static/publications/icdsc08.pdf",extras:[],year:"2008"}],articles2007:[{name:"Integrating Motion, Illumination and Structure in Video Sequences, With Applications in Illumination-Invariant Tracking",note:"Y. Xu and A. Roy-Chowdhury, IEEE Trans. on Pattern Analysis and Machine Intelligence, May 2007.",link:"static/publications/stamp (1).html",extras:[],year:"2007"},{name:"Towards A Measure of Deformability of Shape Sequences",note:"A. Roy-Chowdhury, Pattern Recognition Letters, Vol. 28, 2007.",link:"static/publications/shape_prl.pdf",extras:[],year:"2007"},{name:"Stochastic Adaptive Tracking In A Camera Network",note:"B. Song, A. Roy-Chowdhury, IEEE Intl. Conf. on Computer Vision, 2007.",link:"static/publications/iccv07.pdf",extras:[],year:"2007"},{name:"Closed-loop Tracking and Change Detection in Multi-Activity Sequences",note:"B. Song, N. Vaswani, A. Roy-Chowdhury, IEEE Computer Vision and Pattern Recognition, 2007.",link:"static/publications/cvpr07.pdf",extras:[],year:"2007"},{name:"Pose and Illumination Invariant Face Recognition Using Video Sequences",note:"A. Roy-Chowdhury, Y. Xu, Multi-Biometric Systems for Identity Recognition: Theory and Experiments, (Eds. R. Hammoud, M. Abidi and B. Abidi), Springer-Verlag, 2007.",link:"static/publications/face-chapter.pdf",extras:[],year:"2007"},{name:"Super-resolved Facial Texture Under Changing Pose and Illumination",note:"J. Yu, B. Bhanu, Y. Xu, A. Roy-Chowdhury, IEEE Intl. Conf. on Image Processing, 2007.",link:"static/publications/icip07-face.pdf",extras:[],year:"2007"}],articles2006:[{name:"The Function Space of an Activity",note:"A. Veeraraghavan, R. Chellappa, A. Roy-Chowdhury, IEEE Computer Vision and Pattern Recognition, 2006.",link:"static/publications/cvpr06.pdf",extras:[],year:"2006"},{name:"A Multi-Terminal Model-Based Video Compression Algorithm",note:"B. Song, A. Roy-Chowdhury, E. Tuncel, IEEE Intl. Conf. on Image Processing, 2006.",link:"static/publications/icip06-distributed.pdf",extras:[],year:"2006"},{name:"Towards A Multi-Terminal Video Compression Algorithm Using Epipolar Geometry",note:"B. Song, O. Bursalioglu, E. Tuncel, A. Roy-Chowdhury, IEEE Intl. Conf. on Acoustics, Speech and Signal Processing, 2006. (Best Student Paper Award)",link:"static/publications/icassp06.pdf",extras:[],year:"2006"}],articles2005:[{name:"Matching Shape Sequences in Video with Applications in Human Motion Analysis",note:"A. Veeraraghavan, A. Roy-Chowdhury, R. Chellappa, IEEE Trans. on Pattern Analysis and Machine Intelligence, pp. 1896-1909, December, 2005.",link:"static/publications/Comp_Shape_Seq_final.pdf",extras:[],year:"2005"},{name:'"Shape Activity" : A Continuous State HMM for Moving/Deforming Shapes with Application to Abnormal Activity Detection',note:"N. Vaswani, A. Roy-Chowdhury, R. Chellappa, IEEE Trans. on Image Processing, pp. 1603-1616, October, 2005.",link:"static/publications/tip_activity.pdf",extras:[],year:"2005"},{name:"Statistical Bias in 3D Reconstruction from A Monocular Video",note:"Amit K. Roy-Chowdhury, R. Chellappa, IEEE Trans. on Image Processing, pp. 1057-1062, August, 2005.",link:"static/publications/bias_pap.pdf",extras:[],year:"2005"},{name:"Integrating the Effects of Motion, Illumination and Structure in Video Sequences",note:"Y. Xu, A. Roy-Chowdhury. IEEE Intl. Conf. on Computer Vision, 2005.",link:"static/publications/iccv05.pdf",extras:[],year:"2005"},{name:"A Measure of Deformability of Shapes, With Applications to Human Motion Analysis",note:"A. Roy-Chowdhury. IEEE Computer Vision and Pattern Recognition, 2005.",link:"static/publications/cvpr05.pdf",extras:[],year:"2005"},{name:"3D Face Modeling From Monocular Video Sequences",note:"A. Roy-Chowdhury, R. Chellappa, H. Gupta, Face Processing: Advanced Modeling and Methods (Eds. R.Chellappa and W.Zhao), Academic Press, 2005.",link:"static/publications/face-book-chapter.pdf",extras:[],year:"2005"},{name:"Statistics in Computer Vision and Image Processing",note:"R. Chellappa, A Roy-Chowdhury, Encyclopedia of Statistical Sciences, 2nd ed., Vol. 2, (S. Kotz et al., eds.), John Wiley & Sons, Hoboken, pp. 1164-1173, 2005.",link:"static/publications/ess.pdf",extras:[],year:"2005"}],articles2004:[{name:"Identification of Humans Using Gait",note:"A. Kale, A.N. Rajagopalan, A. Sunderesan, N. Cuntoor, A. Roy-Chowdhury, V. Krueger, R. Chellappa, IEEE Trans. on Image Processing, pp. 1163-1173, Sept. 2004.",link:"static/publications/iptrans_gait.pdf",extras:[],year:"2004"},{name:"An Information Theoretic Criterion for Evaluating the Quality of 3D Reconstructions from Video",note:"Amit K. Roy-Chowdhury, R. Chellappa, IEEE Trans. on Image Processing, pp. 960-973, July 2004.",link:"static/publications/iptrans04_info.pdf",extras:[],year:"2004"},{name:"Wide Baseline Image Registration with Application to 3D Face Modeling",note:"Amit K. Roy-Chowdhury, R. Chellappa, Trish Keaton, IEEE Transactions on Multimedia, pp. 423-434, June 2004.",link:"static/publications/mm_trans.pdf",extras:[],year:"2004"},{name:"Role of Shape and Kinematics in Human Movement Analysis",note:"A. Veeraraghavan, A. Roy-Chowdhury, R. Chellappa. IEEE Computer Vision and Pattern Recognition, 2004.",link:"static/publications/cvpr04.pdf",extras:[],year:"2004"},{name:"Multiple View Tracking of Human Motion Modeled by Kinematic Chains",note:"A. Sundaresan, A. Roy-Chowdhury, R. Chellappa, IEEE Int. Conf. on Image Processing, 2004.",link:"static/publications/kinematic-model.pdf",extras:[],year:"2004"},{name:"A System Identification Approach for Video-Based Face Recognition",note:"G. Aggarwal, Amit K. Roy-Chowdhury, R. Chellappa. International Conference on Pattern Recognition, 2004.",link:"static/publications/icpr04.pdf",extras:[],year:"2004"}],articles2003:[{name:"Stochastic Approximation and Rate Distortion Analysis for Robust Structure and Motion Estimation",note:"Amit K. Roy-Chowdhury, R. Chellappa, International Journal on Computer Vision. Volume 55(1), pp. 27-53, October 2003.",link:"static/publications/ijcv_paper.pdf",extras:[],year:"2003"},{name:"Face Reconstruction From Video Using Uncertainty Analysis and a Generic Model",note:"Amit K. Roy-Chowdhury, R. Chellappa, Computer Vision and Image Understanding, 91(1-2), pp. 188-213, July-August 2003.",link:"static/publications/cviu-face.pdf",extras:[],year:"2003"},{name:"Activity Recognition Using the Dynamics of the Configuration of Interacting Objects Namrata Vaswani",note:"Amit K. Roy-Chowdhury, R. Chellappa. IEEE Computer Vision and Pattern Recognition, 2003.",link:"static/publications/hai.pdf",extras:[],year:"2003"},{name:"A Factorization Approach to Activity Recognition",note:"Amit K. Roy-Chowdhury, R. Chellappa. CVPR Workshop on Event Mining, 2003.",link:"static/publications/cvpr03-wrkshp.pdf",extras:[],year:"2003"},{name:"Towards A View Invariant Gait Recognition Algorithm",note:"Amit K. Roy-Chowdhury, A. Kale, R. Chellappa. IEEE Intl. Conf. on Advanced Video and Signal Based Surveillance, 2003.",link:"static/publications/avss03.pdf",extras:[],year:"2003"},{name:"Deterministic and Statistical Properties of Multi-resolution 3D Modeling",note:"A. Roy-Chowdhury, H. Liu, R. Chellappa. ICCV Workshop on Statistical and Computational Theories in Vision, 2003.",link:"static/publications/multi_sctv.pdf",extras:[],year:"2003"}],magazines:[{name:'"About Face" - National Geographic, November 2003.',note:"Work on Face Recognition under pose, illumination variations and disguise was featured in this issue (Pages 18-19). The box on the right of the above link illustrates our work.",link:"static/publications/index.html",year:"2003"}],papers:[{name:"Statistical Analysis of 3D Modeling From Monocular Video Streams",note:"",link:"static/publications/dissertation.pdf",year:"2002"}]}}}},FEEd:function(t,e,a){"use strict";e.a={name:"HumanRobots",data:function(){return{showActiveLearning:!1,showActiveLearningPublications:!1,showCamera:!1,showCameraPublications:!1,activeLearningPublications:[{name:"Non-Uniform Subset Selection for Active Learning in Structured Data",note:"S. Paul, J. H. Bappy, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2017.",link:"static/publications/cvpr2017subset.pdf",year:"2017"},{name:"The Impact of Typicality for Informative Representative Selection",note:"J. H. Bappy, S. Paul, E. Tuncel and A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2017.",link:"static/publications/cvpr2017typicality.pdf",year:"2017"},{name:"Continuous adaptation of multi-camera person identification models through sparse non-redundant representative selection",note:"A. Das, R. Panda, A. Roy-Chowdhury, Computer Vision and Image Understanding, 2016.",link:"static/publications/CVIU_2016_Abir.pdf",extras:[{name:"Supplemental Material",path:"static/publications/Supplementary_CVIU_2016_Abir.pdf"}],year:"2016"},{name:"Online Adaptation for Joint Scene and Object Classification",note:"J. H. Bappy, S. Paul, A. Roy-Chowdhury, European Conf. on Computer Vision, 2016.",link:"static/publications/eccv2016_jawad.pdf",year:"2016"},{name:"Temporal Model Adaptation for Person Re-Identification",note:"N. Martinel, C. Micheloni, A. Roy-Chowdhury, European Conf. on Computer Vision, 2016.",link:"static/publications/niki-eccv16.pdf",year:"2016"},{name:"Generating Diverse Image Datasets with Limited Labeling",note:"N. C. Mithun, R. Panda, A. Roy-Chowdhury, ACM International Conf. on Multimedia, 2016.",link:"static/publications/ACM_2016.pdf",year:"2016"},{name:"Context-Aware Activity Recognition and Anomaly Detection in Video",note:"Y. Zhu, N. Nayak, A. Roy-Chowdhury, IEEE Journal on Selected Topics in Signal Processing, Special Issue on Anomalous Pattern Discovery, February 2013.",link:"static/publications/jstsp13.pdf",extras:[{name:"Code",path:"static/publications/download (2).html"}],year:"2013"},{name:"A Continuous Learning Framework for Activity Recognition Using Deep Hybrid Feature Models",note:"M. Hasan, A. Roy-Chowdhury, IEEE Trans. on Multimedia, 2015.",link:"static/publications/tmm2015.pdf",extras:[{name:"Code",path:"static/publications/hybrid.html"}],year:"2015"},{name:"Context Aware Active Learning of Activity Recognition Models",note:"M. Hasan, A. Roy-Chowdhury, International Conference on Computer Vision, 2015.",link:"static/publications/ICCV2015.pdf",extras:[{name:"Code",path:"static/publications/caal.html"}],year:"2015"},{name:"Continuous Learning of Human Activity Models Using Deep Nets",note:"M. Hasan, A. Roy-Chowdhury, European Conf. on Computer Vision, 2014.",link:"static/publications/eccv2014-1.pdf",extras:[{name:"Code",path:"static/publications/hybrid (1).html"}],year:"2014"},{name:"Incremental Activity Modeling and Recognition in Streaming Videos",note:"M. Hasan, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2014.",link:"static/publications/cvpr2014.pdf",extras:[],year:"2014"}],cameraPublications:[{name:"Weakly Supervised Summarization of Web Videos",note:"R. Panda, A. Das, Z. Wu, J. Ernst and A. Roy-Chowdhury, International Conference on Computer Vision, 2017.",link:"static/publications/ICCV_Jawad.pdf",year:"2017"},{name:"Collaborative Summarization of Topic-Related Videos",note:"R. Panda and A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2017.",link:"static/publications/cvpr2017summ.pdf",year:"2017"},{name:"Context-Aware Video Summarization",note:"S. Zhang, Y. Zhu, A. Roy-Chowdhury, IEEE Trans. on Image Processing, 2016.",link:"static/publications/TIP2016_summarization.pdf",year:"2016"}]}}}},Femo:function(t,e,a){"use strict";var i=a("wHfC"),n=a("VU/8"),s=n(null,i.a,null,null,null);e.a=s.exports},Fs8J:function(t,e,a){"use strict";var i=a("OiVu"),n=a("IeM7"),s=a("0sUC"),o=a("OBbp");e.a={components:{AmitHomeBody:i.a,NewsAmit:n.a,PromoImagesAmit:s.a,Sponsors:o.a},name:"AmitHeader"}},"Fzf/":function(t,e,a){t.exports=a.p+"static/img/summarization.e2326bf.png"},G6e4:function(t,e,a){"use strict";function i(t){a("yz8x")}var n=a("E5uT"),s=a("DiwX"),o=a("VU/8"),r=i,l=o(n.a,s.a,r,"data-v-37ac0f88",null);e.a=l.exports},GPKu:function(t,e,a){"use strict";var i=a("hlG1"),n=a("VU/8"),s=n(null,i.a,null,null,null);e.a=s.exports},IeM7:function(t,e,a){"use strict";var i=a("tPp8"),n=a("UWUf"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.a=o.exports},JeFR:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement;t._self._c;return t._m(0)},n=[function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{staticClass:"row text-center",attrs:{id:"sponsors"}},[i("h3",[t._v("Acknowledgements")]),t._v(" "),i("p",[t._v("The Video Computing Group graciously acknowledges the funding received from a number of government agencies and private corporations.")]),t._v(" "),i("div",{staticClass:"col-md-1 col-md-offset-1"},[i("a",{attrs:{href:"#"}},[i("img",{staticClass:"news",attrs:{src:a("gqVA"),alt:"AFOSR"}})])]),t._v(" "),i("div",{staticClass:"col-md-1"},[i("a",{attrs:{href:"#"}},[i("img",{staticClass:"news",attrs:{src:a("MjAs"),alt:"ARO"}})])]),t._v(" "),i("div",{staticClass:"col-md-1"},[i("a",{attrs:{href:"#"}},[i("img",{staticClass:"news",attrs:{src:a("Muom"),alt:"CISCO"}})])]),t._v(" "),i("div",{staticClass:"col-md-1"},[i("a",{attrs:{href:"#"}},[i("img",{staticClass:"news",attrs:{src:a("xq7R"),alt:"DARPA"}})])]),t._v(" "),i("div",{staticClass:"col-md-1"},[i("a",{attrs:{href:"#"}},[i("img",{staticClass:"news",attrs:{src:a("Dd1x"),alt:"Google"}})])]),t._v(" "),i("div",{staticClass:"col-md-1"},[i("a",{attrs:{href:"#"}},[i("img",{staticClass:"news",attrs:{src:a("lGGH"),alt:"Lockheed Martin"}})])]),t._v(" "),i("div",{staticClass:"col-md-1"},[i("a",{attrs:{href:"#"}},[i("img",{staticClass:"news",attrs:{src:a("l+Fy"),alt:"Mayachitra"}})])]),t._v(" "),i("div",{staticClass:"col-md-1"},[i("a",{attrs:{href:"#"}},[i("img",{staticClass:"news",attrs:{src:a("1OOY"),alt:"NEH"}})])]),t._v(" "),i("div",{staticClass:"col-md-1"},[i("a",{attrs:{href:"#"}},[i("img",{staticClass:"news",attrs:{src:a("seTK"),alt:"NGA"}})])]),t._v(" "),i("div",{staticClass:"col-md-1"},[i("a",{attrs:{href:"#"}},[i("img",{staticClass:"news",attrs:{src:a("X6+c"),alt:"NSF"}})])])])}],s={render:i,staticRenderFns:n};e.a=s},K7ZM:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement;t._self._c;return t._m(0)},n=[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"inherit-height",attrs:{id:"data"}},[a("div",{attrs:{id:"home-body"}},[a("div",{staticClass:"container row"},[a("div",{staticClass:"col-md-6 dataset"},[a("h2",[t._v("Videoweb Activities Dataset")]),t._v(" "),a("p",[t._v("The Videoweb Activities Dataset has about 2.5 hours of video data consisting of dozens of activities along with annotation. The data is now available publicly for research. Please download and submit the following release form to gain access to the dataset. Below are some samples from the data.")]),t._v(" "),a("a",{staticClass:"btn btn-primary",attrs:{href:"../../static/datasets/RELEASE_OF_VIDEWEB_ACTIVITIES_DATASET.pdf"}},[t._v("Release Form")]),t._v(" "),a("a",{staticClass:"btn btn-primary",attrs:{href:"https://svn.engr.ucr.edu/vcSVN/VideoWebData"}},[t._v("Login to Videoweb Activities Dataset")]),t._v(" "),a("p",[a("strong",[t._v("Please refer to the following article when using this dataset:")]),a("br"),t._v(" "),a("small",[t._v('G. Denina, B. Bhanu, H. Nguyen, C. Ding, A. Kamal, C. Ravishankar, A. Roy-Chowdhury, A. Ivers, and B. Varda, "VideoWeb Dataset for Multi-camera Activities and Non-verbal Communication", in Distributed Video Sensor Networks (Eds. B. Bhanu, C. Ravishankar, A. Roy-Chowdhury, H. Aghajan, D. Terzopoulos), Springer 2010.\n              This data collection was partially supported by the Aware Building project under ONR-N00014-07-C-0311 and ARO grant W911NF-07-1-0485.')])]),t._v(" "),a("a",{staticClass:"btn btn-info",attrs:{href:"../../static/datasets/scene1.mpg"}},[t._v("Sample Scene 1")]),t._v(" "),a("a",{staticClass:"btn btn-info",attrs:{href:"../../static/datasets/scene2.mpg"}},[t._v("Sample Scene 2")]),t._v(" "),a("a",{staticClass:"btn btn-info",attrs:{href:"../../static/datasets/scene3.mpg"}},[t._v("Sample Scene 3")]),t._v(" "),a("a",{staticClass:"btn btn-info",attrs:{href:"../../static/datasets/scene4.mpg"}},[t._v("Sample Scene 4")])]),t._v(" "),a("div",{staticClass:"col-md-6 dataset"},[a("h2",[t._v("Re-identification Across indoor-outdoor Dataset (RAiD)")]),t._v(" "),a("p",[t._v("This person re-identification dataset was collected at the Winstun Chung Hall of UC Riverside. It is a 4 camera dataset with 2 indoor and 2 outdoor cameras. The cameras are numbered as 1,2,3 and 4 where cameras 1 and 2 are indoor while cameras 3 and 4 are outdoor. 43 people walked in these camera views resulting in 6920 images. Among the 43 persons 41 people appeared in all the 4 cmareas where as person 8 is not present in camera 3 and person 34 is not present in camera 4.")]),t._v(" "),a("a",{staticClass:"btn btn-primary",attrs:{href:"http://cs-people.bu.edu/dasabir/raid.php"}},[t._v("Dataset")]),t._v(" "),a("p",[a("strong",[t._v("Please refer to the following article when using this dataset:")]),a("br"),t._v(" "),a("small",[t._v('\n                      A. Das, A. Chakraborty, A. Roy-Chowdhury."Consistent Re-identification In A Camera Network". European Conference on Computer Vision, pp. 330-345, vol.8690, Zurich, 2014.\n                  ')])])])]),t._v(" "),a("br"),t._v(" "),a("hr",{staticClass:"hr-blue"}),t._v(" "),a("div",{staticClass:"container row"},[a("div",{staticClass:"col-md-6 dataset"},[a("h2",[t._v("Camera Network Tracking Dataset (CamNeT)")]),t._v(" "),a("p",[t._v("CamNeT is a non-overlapping camera network dataset that is designed for tracking. The dataset is composed of five to eight cameras covering both indoor and outdoor scenes at University of California, Riverside. This dataset consists of six scenarios. Within each scenario are challenges relevant to lighting changes, complex topographies, crowded scenes, and changing grouping dynamics. Persons with predefined trajectories are combined with persons with random trajectories. A baseline multi-target tracking system and its results are provided.")]),t._v(" "),a("a",{staticClass:"btn btn-primary",attrs:{href:"https://drive.google.com/open?id=0B7uDdIqGrZlVfmdlOFZyUEg3RHUxaUdSaGVGTTNDT3R0dUNLSFdCSkZYVWk0dE16TFg4cTA&authuser=0"}},[t._v("Dataset")]),t._v(" "),a("p",[a("strong",[t._v("Please refer to the following article when using this dataset:")]),a("br"),t._v(" "),a("small",[t._v('S. Zhang, E. Staudt, T. Faltemier, A. Roy-Chowdhury. "A Camera Network Tracking (CamNeT) Dataset and Performance Baseline". In IEEE Winter Conference on Applications of Computer Vision, Waikoloa Beach, Hawaii, January, 2015.')])])]),t._v(" "),a("div",{staticClass:"col-md-6 dataset"},[a("h2",[t._v("Tour20 Video Summarization Dataset")]),t._v(" "),a("p",[t._v("Tour20 is a video summarization dataset that is designed primarily for multi-video summarization. However, it can also be used for evaluating single-video summarization in a repeatable and efficient way. It contains 140 videos of total 6 hour 46 minutes duration that are downloaded from YouTube with creative commons license, CC-By 3.0. The dataset consists of three human created ground truth summaries for each of the videos as well as a diverse set of summary to describe the video collection of a tourist place. We also provide the shot segmentation files that indicate the shot boundary transitions of each video.")]),t._v(" "),a("a",{staticClass:"btn btn-primary",attrs:{href:"https://drive.google.com/file/d/0B3uIyPMHSpNrSzk5QWc3WnBwT3c/view?pref=2&pli=1"}},[t._v("Dataset")]),t._v(" "),a("p",[a("strong",[t._v("Please refer to the following article when using this dataset:")]),a("br"),t._v(" "),a("small",[t._v('\n                      A. Das, A. Chakraborty, A. Roy-Chowdhury."Consistent Re-identification In A Camera Network". European Conference on Computer Vision, pp. 330-345, vol.8690, Zurich, 2014.\n                  ')])])])]),t._v(" "),a("br"),t._v(" "),a("hr",{staticClass:"hr-blue"}),t._v(" "),a("div",{staticClass:"container row"},[a("div",{staticClass:"col-md-6 dataset"},[a("h2",[t._v("DivNet Image Dataset")]),t._v(" "),a("p",[t._v("The DivNet dataset contains images for around 550 object and scene categories, averaging around 1K images per category. The categories are mainly chosen from ILSVRC2016 object detection and scene classification challenge. The images for each category were originally collected from Google, Bing and Flickr. The collected images has been refined in a semi-supervised incremental sparse-coding framework, so that a high-quality image dataset can be created with limited human labeling. We release images, which were originally crawled with Creative Commons license (CC-By 3.0) for non-commercial reuse.")]),t._v(" "),a("a",{staticClass:"btn btn-primary",attrs:{href:"https://drive.google.com/drive/folders/0Bznxmj1nmRUjSC1LSzE0aTctUzA"}},[t._v("Dataset")]),t._v(" "),a("p",[a("strong",[t._v("Please refer to the following article when using this dataset:")]),a("br"),t._v(" "),a("small",[t._v('N. C. Mithun, R. Panda and A. K. Roy-Chowdhury, "Generating Diverse Image Datasets with Limited Labeling" in ACM MM, Amsterdam, October, 2016.')])])])])])])}],s={render:i,staticRenderFns:n};e.a=s},M93x:function(t,e,a){"use strict";var i=a("xJD8"),n=a("djTn"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.a=o.exports},MjAs:function(t,e,a){t.exports=a.p+"static/img/aro.6ae0755.gif"},Muom:function(t,e,a){t.exports=a.p+"static/img/cisco.1ebec25.png"},"N+zL":function(t,e,a){"use strict";Object.defineProperty(e,"__esModule",{value:!0});var i=a("+p9J"),n=a("rKk8"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.default=o.exports},NHnr:function(t,e,a){"use strict";Object.defineProperty(e,"__esModule",{value:!0});var i=a("7+uW"),n=a("M93x"),s=a("YaEn"),o=a("O0Ew"),r=a("+YSy"),l=a("GPKu"),c=a("F3EI"),d=a.n(c);a("v2ns"),i.a.component("Header",o.a),i.a.component("Nav",r.a),i.a.component("Footer",l.a),i.a.config.productionTip=!1,i.a.use(d.a),new i.a({el:"#app",router:s.a,template:"<App/>",components:{App:n.a}})},Nf5o:function(t,e,a){"use strict";e.a={data:function(){return{teachings:[{number:"EE114",name:"Probability, Random Variables and Processes in Electrical Engineering",type:"Undergraduate Core Class"},{number:"EE215",name:"Stochastic Processes",type:"Graduate Core Class"},{number:"EE236",name:"State and Parameter Estimation",type:"Graduate Elective Class"},{number:"EE241",name:"Advanced Digital Image Processing",type:"Graduate Elective Class"},{number:"EE243",name:"Advanced Computer Vision",type:"Graduate Elective Class"},{number:"EE247",name:"Current Topics in Computer Vision",type:"Graduate Elective Class"}]}}}},O0Ew:function(t,e,a){"use strict";var i=a("vaqm"),n=a("8EWh"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.a=o.exports},OBbp:function(t,e,a){"use strict";var i=a("JeFR"),n=a("VU/8"),s=n(null,i.a,null,null,null);e.a=s.exports},OiVu:function(t,e,a){"use strict";var i=a("iY3p"),n=a("VU/8"),s=n(null,i.a,null,null,null);e.a=s.exports},QeD3:function(t,e,a){t.exports=a.p+"static/img/Continuous-Learning.840f601.png"},R6fH:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"inherit-height"},[a("div",{attrs:{id:"home-body"}},[a("PromoImagesAmit"),t._v(" "),a("AmitHomeBody"),t._v(" "),a("NewsAmit"),t._v(" "),a("Sponsors")],1)])},n=[],s={render:i,staticRenderFns:n};e.a=s},UWUf:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"home-body"},[a("swiper",{attrs:{options:t.swiperOption}},t._l(t.news,function(e){return a("swiper-slide",{key:e.name},[a("a",{attrs:{href:"../../static/publications/"+e.link}},[a("img",{staticClass:"news",attrs:{src:"../../static/img/featured_news/"+e.image,alt:e.name}})]),t._v(" "),a("a",{attrs:{href:"../../static/publications/"+e.link}},[t._v("\n      "+t._s(e.name)+"\n    ")])])}))],1)},n=[],s={render:i,staticRenderFns:n};e.a=s},Upxw:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{staticClass:"inherit-height",attrs:{id:"data"}},[i("div",{attrs:{id:"home-body"}},[i("div",{staticClass:"container row"},[i("div",{staticClass:"col-md-12"},[i("h2",{attrs:{id:"name"}},[t._v("Activity Recognition and Prediction")]),t._v(" "),i("img",{staticClass:"research-img",attrs:{src:a("l34S"),alt:"Graph Structure"}}),t._v(" "),i("p",[t._v("\n               In this project, we investigate the problem of forecasting future activities in continuous videos. Ability to successfully forecast activities that are yet to be observed is a very important video understanding problem, and is starting to receive attention in the computer vision literature. An activity forecasting strategy that models the simultaneous and/or sequential nature of human activities on a graph and combines that with the interrelationship between static scene cues and dynamic target trajectories is explored.\n           ")]),t._v(" "),i("h4",[i("span",{staticClass:"publicationHeader"},[t._v("Sample Publications")]),t._v(" "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showActivityPublications=!t.showActivityPublications}}},[t._v("Toggle")])]),t._v(" "),i("ul",{directives:[{name:"show",rawName:"v-show",value:t.showActivityPublications,expression:"showActivityPublications"}],staticClass:"list-group"},t._l(t.activityPublications,function(e){return i("li",{key:e.name,staticClass:"list-group-item"},[i("a",{attrs:{href:e.link}},[i("h5",[t._v(t._s(e.name))]),t._v(" "),i("p",[i("small",[t._v(t._s(e.note))])])]),t._v(" "),void 0!==e.extras?i("div",{staticClass:"extraContainer"},t._l(e.extras,function(e){return i("a",{key:e.path,staticClass:"btn btn-primary",attrs:{href:e.path}},[t._v(t._s(e.name))])})):t._e()])})),t._v(" "),i("hr"),t._v(" "),i("h3",[t._v("Context-Aware Activity Recognition "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showContent=!t.showContent}}},[t._v("View")])]),t._v(" "),i("div",{directives:[{name:"show",rawName:"v-show",value:t.showContent,expression:"showContent"}]},[i("img",{staticClass:"research-img",attrs:{src:a("el01"),alt:"Context in Activity"}}),t._v(" "),i("p",[t._v("\n                 In this project, we are developing methods for recognition of complex activities in video. A core focus area has been on context-aware modeling and recognition strategies, where neighborhood information is exploited to recognize the activities on the targets of interest. We have also shown the utility of usage statistics in searching large video datasets.\n             ")]),t._v(" "),i("h4",[i("span",{staticClass:"publicationHeader"},[t._v("Sample Publications")]),t._v(" "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showContentPublications=!t.showContentPublications}}},[t._v("Toggle")])]),t._v(" "),i("ul",{directives:[{name:"show",rawName:"v-show",value:t.showContentPublications,expression:"showContentPublications"}],staticClass:"list-group"},t._l(t.contentPublications,function(e){return i("li",{key:e.name,staticClass:"list-group-item"},[i("a",{attrs:{href:e.link}},[i("h5",[t._v(t._s(e.name))]),t._v(" "),i("p",[i("small",[t._v(t._s(e.note))])])]),t._v(" "),void 0!==e.extras?i("div",{staticClass:"extraContainer"},t._l(e.extras,function(e){return i("a",{key:e.path,staticClass:"btn btn-primary",attrs:{href:e.path}},[t._v(t._s(e.name))])})):t._e()])}))])])])])])},n=[],s={render:i,staticRenderFns:n};e.a=s},V7w3:function(t,e,a){"use strict";var i=a("ljyK"),n=a("oRc/"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.a=o.exports},"X6+c":function(t,e,a){t.exports=a.p+"static/img/nsf.94d4b47.png"},XMuo:function(t,e,a){"use strict";var i=a("udlE"),n=a("/Nfc"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.a=o.exports},Xy5h:function(t,e,a){t.exports=a.p+"static/img/image5.bb7892c.png"},Ya1q:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"inherit-height",attrs:{id:"data"}},[a("div",{attrs:{id:"home-body"}},[a("div",{staticClass:"container row"},[a("div",{staticClass:"col-md-12"},[a("h2",{attrs:{id:"name"}},[t._v("Wide Area Scene Analysis in Vision Network")]),t._v(" "),t._m(0),t._v(" "),a("p",[t._v("\n                 Networks of cameras are nowadays installed in various places. However, most of the data collected by such networks are analyzed manually, thus severely limiting their application. In this research, which is divided into different sub-projects, we are studying the core scientific and technical issues related to camera networks, such as information fusion, network control, activity analysis, the interplay between networking and computer vision, and computational complexity. \n             ")]),t._v(" "),a("hr"),t._v(" "),a("h3",[t._v("Camera Network Tracking and Re-identification "),a("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showCameraNetwork=!t.showCameraNetwork}}},[t._v("View")])]),t._v(" "),a("div",{directives:[{name:"show",rawName:"v-show",value:t.showCameraNetwork,expression:"showCameraNetwork"}]},[a("p",[t._v("\n                 In many computer vision tasks it is often desirable to identify and monitor people as they move through a network of non-overlapping cameras. This is especially challenging as for a network of cameras issues such as changes of scale, illumination, viewing angle and pose start to arise. In this project we try to address this inter camera person association problem. We have shown that incorporating a consistency requirement of re-identification results across the camera network can significantly improve the re-identification performance even in the challenging scenarios where the illumination changes between the cameras are large.\n             ")]),t._v(" "),a("h4",[a("span",{staticClass:"publicationHeader"},[t._v("Sample Publications")]),t._v(" "),a("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showCameraPublications=!t.showCameraPublications}}},[t._v("Toggle")])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showCameraPublications,expression:"showCameraPublications"}],staticClass:"list-group"},t._l(t.cameraPublications,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("a",{attrs:{href:e.link}},[a("h5",[t._v(t._s(e.name))]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"},t._l(e.extras,function(e){return a("a",{key:e.path,staticClass:"btn btn-primary",attrs:{href:e.path}},[t._v(t._s(e.name))])})):t._e()])}))]),t._v(" "),a("hr"),t._v(" "),a("h3",[t._v("Distributed Estimation "),a("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showDistributed=!t.showDistributed}}},[t._v("View")])]),t._v(" "),a("div",{directives:[{name:"show",rawName:"v-show",value:t.showDistributed,expression:"showDistributed"}]},[a("p",[t._v("\n                 In this project, we are developing methods for estimation and control in distributed camera networks. Specifically, we have looked at the following problems.\n             ")]),t._v(" "),t._m(1),t._v(" "),a("h4",[t._v("Demos")]),t._v(" "),a("p",[t._v("Information-Weighted Consensus in a Distributed Camera Network")]),t._v(" "),t._m(2),t._v(" "),a("h4",[a("span",{staticClass:"publicationHeader"},[t._v("Sample Publications")]),t._v(" "),a("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showDistributedPublications=!t.showDistributedPublications}}},[t._v("Toggle")])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showDistributedPublications,expression:"showDistributedPublications"}],staticClass:"list-group"},t._l(t.cameraPublications,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("a",{attrs:{href:e.link}},[a("h5",[t._v(t._s(e.name))]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"},t._l(e.extras,function(e){return a("a",{key:e.path,staticClass:"btn btn-primary",attrs:{href:e.path}},[t._v(t._s(e.name))])})):t._e()])}))]),t._v(" "),a("hr"),t._v(" "),a("h3",[t._v("Active Sensing "),a("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showActiveSensing=!t.showActiveSensing}}},[t._v("View")])]),t._v(" "),a("div",{directives:[{name:"show",rawName:"v-show",value:t.showActiveSensing,expression:"showActiveSensing"}]},[a("p",[t._v("\n                 We are developing methods for optimizing the image acquisition capabilities of the cameras so as to maximize the performance of the methods used to analyze these image. This needs to be done through collaboration between the cameras in a network, as each camera's parameters entail constraints on the others. We have developed distributed control methods for collaborative and opportunistic sensing in wide-area camera networks.\n             ")]),t._v(" "),a("h4",[a("span",{staticClass:"publicationHeader"},[t._v("Sample Publications")]),t._v(" "),a("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showActiveSensingPublications=!t.showActiveSensingPublications}}},[t._v("Toggle")])]),t._v(" "),a("ul",{directives:[{name:"show",rawName:"v-show",value:t.showActiveSensingPublications,expression:"showActiveSensingPublications"}],staticClass:"list-group"},t._l(t.activeSensingPublications,function(e){return a("li",{key:e.name,staticClass:"list-group-item"},[a("a",{attrs:{href:e.link}},[a("h5",[t._v(t._s(e.name))]),t._v(" "),a("p",[a("small",[t._v(t._s(e.note))])])]),t._v(" "),void 0!==e.extras?a("div",{staticClass:"extraContainer"},t._l(e.extras,function(e){return a("a",{key:e.path,staticClass:"btn btn-primary",attrs:{href:e.path}},[t._v(t._s(e.name))])})):t._e()])}))]),t._v(" "),a("hr"),t._v(" "),t._m(3)])])])])},n=[function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("video",{attrs:{width:"100%",controls:""}},[i("source",{attrs:{src:a("8oDi"),type:"video/mp4"}}),t._v("\n               Your browser does not support HTML5 video\n           ")])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ul",[a("li",[t._v("Distributed estimation algorithms that are aware of the specific constraints of camera networks")]),t._v(" "),a("li",[t._v("Distributed control for collaborative and opportunistic sensing in wide-area camera networks")])])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ul",[a("li",[a("a",{attrs:{href:"../../../static/img/research/ICF_Matlab_Code.zip"}},[t._v("Matlab code for Distributed Single Target Tracking")])]),t._v(" "),a("li",[a("a",{attrs:{href:"../../../static/img/research/MTIC_Matlab_Code.zip"}},[t._v("Matlab code for Distributed Multi-target Tracking")])])])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("p",[a("em",[t._v("Parts of this work have been supported by the NSF CPS program under the project "),a("a",{attrs:{href:"http://www.vislab.ucr.edu/PROJECTS/CPS/CPS.php"}},[t._v("Distributed Sensing, Learning and Control in Dynamic Environments")]),t._v(".")])])}],s={render:i,staticRenderFns:n};e.a=s},YaEn:function(t,e,a){"use strict";var i=a("7+uW"),n=a("/ocq"),s=a("lO7g"),o=a("1Shb"),r=a("G6e4"),l=a("gJEX"),c=a("lERY"),d=a("6rSW"),u=a("BNYb"),p=a("vt/o"),m=a("hrZh"),h=a("V7w3"),v=a("XMuo"),g=a("rf4J"),f=a("Femo"),C=a("1wL/");i.a.use(n.a),e.a=new n.a({routes:[{path:"/",name:"Home",component:s.a},{path:"/datasets",name:"Datasets",component:o.a},{path:"/publications",name:"Publications",component:r.a},{path:"/people",name:"PeopleHome",component:l.a},{path:"/amit",name:"Amit",component:c.a},{path:"/HumanRobots",name:"Human Robots",component:u.a},{path:"/WideArea",name:"Wide Area",component:p.a},{path:"/ActivityRP",name:"Activity Recognition and Prediction",component:m.a},{path:"/SituationalAwareness",name:"Situational Awareness",component:h.a},{path:"/FaceTR",component:v.a},{path:"/BiologicalIA",component:g.a},{path:"/VideoWeb",component:f.a},{path:"/MTVideo",component:C.a},{path:"/positions",component:d.a}],mode:"history"})},bylR:function(t,e,a){"use strict";e.a={data:function(){return{showActivityPublications:!1,activityPublications:[{name:"Joint Prediction of Activity Labels and Starting Times in Untrimmed Videos",note:"T. Mahmud, M. Hasan and A. Roy-Chowdhury, International Conference on Computer Vision, 2017.",link:"static/publications/ICCV_Tahmida.pdf",year:"2017"},{name:"Learning Temporal Regularity in Video Sequences",note:"M. Hasan, J. Choi, J. Neumann, A. Roy-Chowdhury, and L. Davis, IEEE Conf. on Computer Vision and Pattern Recognition, 2016.",link:"static/publications/cvpr2016_regularity.pdf",extras:[{name:"Code",path:"static/publications/regularity.html"}],year:"2016"},{name:"A Poisson Process Model for Activity Forecasting",note:"T. Mahmud, M. Hasan, A. Chakraborty, A. Roy-Chowdhury, IEEE International Conf. on Image Processing, 2016.",link:"static/publications/icip2016_timeforecast.pdf",year:"2016"},{name:"Context-Aware Activity Forecasting",note:"A. Chakraborty, A. Roy-Chowdhury, Asian Conf. on Computer Vision, 2014.",link:"static/publications/ACCV_2014.pdf",extras:[],year:"2014"}],showContent:!1,showContentPublications:!1,contentPublications:[{name:"Context Aware Active Learning of Activity Recognition Models",note:"M. Hasan, A. Roy-Chowdhury, International Conference on Computer Vision, 2015.",link:"static/publications/ICCV2015.pdf",extras:[{name:"Code",path:"static/publications/caal.html"}],year:"2015"},{name:"Hierarchical Graphical Models for Simultaneous Tracking and Recognition in Wide-Area Scenes",note:"N. M. Nayak, Y. Zhu, and A. K. Roy-Chowdhury, IEEE Transactions on Image Processing, 2015.",link:"static/publications/tip_twocolumn_revised.pdf",year:"2015"},{name:"Context-Aware Activity Modeling using Hierarchical Conditional Random Fields",note:"Y. Zhu, N. Nayak, A. Roy-Chowdhury, IEEE Trans. on Pattern Analysis and Machine Intelligence, 2015.",link:"static/publications/PAMI14-Activity.pdf",extras:[{name:"Code",path:"static/publications/download.html"}],year:"2015"},{name:'Modeling Multi-object Interactions using "String of Feature Graphs"',note:"Y. Zhu, N. Nayak, U. Gaur, B. Song, A. Roy-Chowdhury, Computer Vision and Image Understanding, 2013.",link:"static/publications/cviu13.pdf",extras:[],year:"2013"},{name:"Context-Aware Activity Recognition and Anomaly Detection in Video",note:"Y. Zhu, N. Nayak, A. Roy-Chowdhury, IEEE Journal on Selected Topics in Signal Processing, Special Issue on Anomalous Pattern Discovery, February 2013.",link:"static/publications/jstsp13.pdf",extras:[{name:"Code",path:"static/publications/download (2).html"}],year:"2013"},{name:"Vector Field Analysis for Multi-Object Behavior Modeling",note:"N. Nayak, Y. Zhu, A. Roy-Chowdhury, Image and Vision Computing, 2013.",link:"static/publications/N. Nayak, Y. Zhu, A. Roy-Chowdhury, Image and Vision Computing, 2013.",extras:[],year:"2013"},{name:'A "String of Feature Graphs" Model for Recognition of Complex Activities in Natural Videos',note:"U. Gaur, Y. Zhu, B. Song, A. Roy-Chowdhury, IEEE Conf. on Computer Vision, 2011.",link:"static/publications/iccv-SFG.pdf",extras:[],year:"2011"},{name:"Features with Feeling - Incorporating User Preferences in Video Categorization",note:"R. Srinivasan, A. Roy-Chowdhury, Asian Conference on Computer Vision, 2012.",link:"static/publications/accv-2012-final.pdf",extras:[],year:"2012"}]}}}},cZ8c:function(t,e,a){t.exports=a.p+"static/img/image3.e20f56f.png"},"dQ+c":function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{staticClass:"inherit-height",attrs:{id:"data"}},[i("div",{attrs:{id:"home-body"}},[i("div",{staticClass:"container row"},[i("div",{staticClass:"col-md-12"},[i("h2",{attrs:{id:"name"}},[t._v("Multi-terminal Video Compression")]),t._v(" "),i("img",{staticClass:"research-img",attrs:{src:a("mewy"),alt:"Pictorial description of the proposed correspondence tracking algorithm. The numbers in circles indicate the steps of the algorithm."}}),t._v(" "),t._m(0),t._v(" "),i("h4",[i("span",{staticClass:"publicationHeader"},[t._v("Sample Publications")]),t._v(" "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showVideoPublications=!t.showVideoPublications}}},[t._v("Toggle")])]),t._v(" "),i("ul",{directives:[{name:"show",rawName:"v-show",value:t.showVideoPublications,expression:"showVideoPublications"}],staticClass:"list-group"},t._l(t.videoPublications,function(e){return i("li",{key:e.name,staticClass:"list-group-item"},[i("a",{attrs:{href:e.link}},[i("h5",[t._v(t._s(e.name))]),t._v(" "),i("p",[i("small",[t._v(t._s(e.note))])])]),t._v(" "),void 0!==e.extras?i("div",{staticClass:"extraContainer"},t._l(e.extras,function(e){return i("a",{key:e.path,staticClass:"btn btn-primary",attrs:{href:e.path}},[t._v(t._s(e.name))])})):t._e()])}))])])])])},n=[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("p",[t._v("\n               Traditional video compression deals with encoding a video sequence after removing the spatial redundancy in each video frame and the temporal redundancy between the frames. On the other hand, if we have multiple video sequences from different cameras where there is a significant overlap between the sequences, these methods are inefficient since they do not consider the redundancy between the video sequences at different sensors. In our research, we develop a distributed video compression scheme that can exploit this inter-sensor redundancy, with minimal communication between the sensors. The scheme is based on transform coding of distributed sources and exploiting the geometrical relationships between the locations of the sensors. The geometry is used to align the video sequences and distributed quantization of transform coefficients is used to eliminate spatial and inter-sensor redundancy. We develop a Distributed Motion Estimation (DME) algorithm that combines traditional block-matching based motion estimation with the epipolar geometry of two cameras. Results demonstrate that our algorithm yields a significant saving in bit rate on the overlapping portion of multiple views. The project involved active collaboration with "),a("a",{attrs:{href:"http://www.ee.ucr.edu/~ertem/"}},[t._v("Prof. E. Tuncel of Electrical Engineering at UCR")]),t._v(".\n             ")])}],s={render:i,staticRenderFns:n};e.a=s},djTn:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"container inherit-height",attrs:{id:"app"}},[a("Header"),t._v(" "),a("Nav"),t._v(" "),a("router-view"),t._v(" "),a("Footer")],1)},n=[],s={render:i,staticRenderFns:n};e.a=s},dmKQ:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"inherit-height",attrs:{id:"data"}},[a("div",{attrs:{id:"home-body"}},[a("div",{staticClass:"container row"},[t._m(0),t._v(" "),a("div",{staticClass:"col-md-6"},[a("div",{staticClass:"row"},t._l(t.facultyMembers,function(e){return a("div",{key:e.name,staticClass:"person"},[a("div",{key:e.name,staticClass:"col-md-3"},[a("img",{staticClass:"img-responsive",attrs:{src:"static/img/people/"+e.image}})]),t._v(" "),a("div",{staticClass:"col-md-9"},[a("h4",[a("strong",[t._v(t._s(e.name))])]),t._v(" "),a("p",[a("strong",[t._v(t._s(e.position))])])]),t._v(" "),a("div",{staticClass:"col-md-12 personActions"},[e.contact.email.length>0?a("a",{staticClass:"btn btn-primary btn-sm",attrs:{href:"mailto:"+e.contact.email}},[a("span",{staticClass:"fa fa-envelope-o"}),t._v(" Email")]):t._e(),t._v(" "),e.contact.website.length>0?a("a",{staticClass:"btn btn-primary btn-sm",attrs:{href:e.contact.website}},[a("span",{staticClass:"fa fa-globe"}),t._v(" Website")]):t._e()])])}))])]),t._v(" "),a("div",{staticClass:"container row"},[t._m(1),t._v(" "),t._l(t.grads,function(e){return a("div",{key:e.name,staticClass:"col-md-6 personContainer"},[a("div",{staticClass:"row"},[a("div",{staticClass:"person"},[a("div",{key:e.name,staticClass:"col-md-3"},[a("img",{staticClass:"img-responsive",attrs:{src:"static/img/people/"+e.image}})]),t._v(" "),a("div",{staticClass:"col-md-9"},[a("h4",[a("strong",[t._v(t._s(e.name))])]),t._v(" "),a("p",[a("strong",[t._v(t._s(e.position))])])]),t._v(" "),a("div",{staticClass:"col-md-12 personActions"},[e.contact.email.length>0?a("a",{staticClass:"btn btn-primary btn-sm",attrs:{href:"mailto:"+e.contact.email}},[a("span",{staticClass:"fa fa-envelope-o"}),t._v(" Email")]):t._e(),t._v(" "),e.contact.website.length>0?a("a",{staticClass:"btn btn-primary btn-sm",attrs:{href:e.contact.website}},[a("span",{staticClass:"fa fa-globe"}),t._v(" Website")]):t._e()])])])])})],2),t._v(" "),a("div",{staticClass:"container row"},[t._m(2),t._v(" "),t._l(t.undergrads,function(e){return a("div",{key:e.name,staticClass:"col-md-6 personContainer"},[a("div",{staticClass:"row"},[a("div",{staticClass:"person"},[a("div",{key:e.name,staticClass:"col-md-3"},[a("img",{staticClass:"img-responsive",attrs:{src:"static/img/people/"+e.image}})]),t._v(" "),a("div",{staticClass:"col-md-9"},[a("h4",[a("strong",[t._v(t._s(e.name))])]),t._v(" "),a("p",[a("strong",[t._v(t._s(e.position))])]),t._v(" "),e.contact.mobile.length>0?a("p",{attrs:{disabled:""}},[a("span",{staticClass:"fa fa-phone"}),t._v(" "+t._s(e.contact.mobile))]):t._e()]),t._v(" "),a("div",{staticClass:"col-md-12 personActions"},[e.contact.email.length>0?a("a",{staticClass:"btn btn-primary btn-sm",attrs:{href:"mailto:"+e.contact.email}},[a("span",{staticClass:"fa fa-envelope-o"}),t._v(" Email")]):t._e(),t._v(" "),e.contact.website.length>0?a("a",{staticClass:"btn btn-primary btn-sm",attrs:{href:e.contact.website}},[a("span",{staticClass:"fa fa-globe"}),t._v(" Website")]):t._e()])])])])})],2),t._v(" "),a("div",{staticClass:"container row"},[t._m(3),t._v(" "),t._l(t.visitors,function(e){return a("div",{key:e.name,staticClass:"col-md-6 personContainer"},[a("div",{staticClass:"row"},[a("div",{staticClass:"person"},[a("div",{key:e.name,staticClass:"col-md-3"},[a("img",{staticClass:"img-responsive",attrs:{src:"static/img/people/"+e.image}})]),t._v(" "),a("div",{staticClass:"col-md-9"},[a("h4",[a("strong",[t._v(t._s(e.name))])]),t._v(" "),a("p",[a("strong",[t._v(t._s(e.position))])]),t._v(" "),e.contact.mobile.length>0?a("p",{attrs:{disabled:""}},[a("span",{staticClass:"fa fa-phone"}),t._v(" "+t._s(e.contact.mobile))]):t._e()]),t._v(" "),a("div",{staticClass:"col-md-12 personActions"},[e.contact.email.length>0?a("a",{staticClass:"btn btn-primary btn-sm",attrs:{href:"mailto:"+e.contact.email}},[a("span",{staticClass:"fa fa-envelope-o"}),t._v(" Email")]):t._e(),t._v(" "),e.contact.website.length>0?a("a",{staticClass:"btn btn-primary btn-sm",attrs:{href:e.contact.website}},[a("span",{staticClass:"fa fa-globe"}),t._v(" Website")]):t._e()])])])])})],2),t._v(" "),a("div",{staticClass:"container row"},[t._m(4),t._v(" "),a("ul",t._l(t.alumni,function(e){return a("li",{key:e.name},[a("strong",[t._v(t._s(e.name))]),t._v(" ("+t._s(e.degree)+") "),a("br"),t._v("\n            "+t._s(e.position)+" "),a("br"),t._v(" "),e.thesis.length>0?a("span",[t._v("\n              Thesis: "),e.thesisLink.length>0?a("a",{attrs:{href:"static/thesis/"+e.thesisLink}},[t._v(t._s(e.thesis))]):a("span",[t._v(t._s(e.thesis))])]):t._e()])}))])])])},n=[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"col-md-12 text-center"},[a("h2",[t._v("Faculty Members")])])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"col-md-12 text-center"},[a("h2",[t._v("Graduate Students")])])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"col-md-12 text-center"},[a("h2",[t._v("Undergradute Students")])])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"col-md-12 text-center"},[a("h2",[t._v("Visiting Scholars")])])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"col-md-12 text-center"},[a("h2",[t._v("Former Students and Research Collaborators")])])}],s={render:i,staticRenderFns:n};e.a=s},eJDZ:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"swiper-container"},[t._t("parallax-bg"),t._v(" "),a("div",{class:t.defaultSwiperClasses.wrapperClass},[t._t("default")],2),t._v(" "),t._t("pagination"),t._v(" "),t._t("button-prev"),t._v(" "),t._t("button-next"),t._v(" "),t._t("scrollbar")],2)},n=[],s={render:i,staticRenderFns:n};e.a=s},el01:function(t,e,a){t.exports=a.p+"static/img/context_in_activity.fc15118.png"},fVvD:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{staticClass:"inherit-height",attrs:{id:"data"}},[i("div",{attrs:{id:"home-body"}},[i("div",{staticClass:"container row"},[i("div",{staticClass:"col-md-12"},[i("h2",{attrs:{id:"name"}},[t._v("Human Robot Vision Networks for Scene Understanding")]),t._v(" "),i("p",[t._v("\n               Automated visual scene understanding has remained a very hard problem for uncontrolled environments. For the foreseeable future, it is natural to expect that humans and computer vision systems will be working together in applications like disaster response. This project explores the fundamental scientific challenges in the coordination between humans and robots for tasks in computer vision and robotics.\n           ")]),t._v(" "),i("p",[t._v("\n               Employing robots, instead of humans, for dangerous tasks in security, surveillance, and disaster response operations is a long-standing goal. However, developing robots that would be capable of carrying out all necessary tasks (e.g., searching for and tending to victims, neutralizing dangers such as fires) with full autonomy still remains a remote possibility. A more tangible goal is to have robots that will operate alongside humans, to jointly accomplish a given mission. Robots can provide useful information, keep humans out of harm's way, and enhance or enable operations in places too strenuous or dangerous for humans. \n           ")]),t._v(" "),i("img",{staticClass:"research-img",attrs:{src:a("0+xk"),alt:"HCI Network"}}),t._v(" "),i("p",[t._v("\n               Towards meeting these objectives, we propose a research program for wide-area scene understanding using a team comprising humans and robots. The robots will be equipped with sensors (e.g., visual) providing information-rich data that will enable scene understanding. These robots will maneuver, collect data, analyze their data along with information from other team members, and report to the humans the information deemed important, according to specified criteria. The system design will ensure that the decisions, made by each of the robots in a distributed fashion, yield coordination of actions between team members, human or robotic. In this project, we use the term Human-Robot Visual Network (HRVN) to refer to the team employed for the task of scene understanding. The vision sensors will be pan-tilt-zoom (PTZ) cameras with the ability to capture high-fidelity images through active control of the PTZ parameters and positioning of the mobile platform.\n           ")]),t._v(" "),i("hr"),t._v(" "),i("h3",[t._v("Active Learning "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showActiveLearning=!t.showActiveLearning}}},[t._v("View")])]),t._v(" "),i("div",{directives:[{name:"show",rawName:"v-show",value:t.showActiveLearning,expression:"showActiveLearning"}]},[i("img",{staticClass:"research-img",attrs:{src:a("QeD3"),alt:"Continuous Learning"}}),t._v(" "),i("p",[t._v("\n                 Most of the state-of-the-art approaches to human activity recognition in video need an intensive training stage and assume that all of the training examples are labeled and available beforehand. They also use hand-crafted features. These assumptions are unrealistic for many applications where we have to deal with streaming videos. In these videos, as new activities are seen, they can be leveraged upon to improve the current activity recognition models. Under this project, we develop an incremental activity learning framework that is able to continuously update the activity models and learn new ones as more videos are seen. Our proposed approach leverages upon state-of-the-art machine learning tools, most notably active learning and deep learning. It does not require tedious manual labeling of every incoming example of each activity class.\n             ")]),t._v(" "),i("h4",[i("span",{staticClass:"publicationHeader"},[t._v("Sample Publications")]),t._v(" "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showActiveLearningPublications=!t.showActiveLearningPublications}}},[t._v("Toggle")])]),t._v(" "),i("ul",{directives:[{name:"show",rawName:"v-show",value:t.showActiveLearningPublications,expression:"showActiveLearningPublications"}],staticClass:"list-group"},t._l(t.activeLearningPublications,function(e){return i("li",{key:e.name,staticClass:"list-group-item"},[i("a",{attrs:{href:e.link}},[i("h5",[t._v(t._s(e.name))]),t._v(" "),i("p",[i("small",[t._v(t._s(e.note))])])]),t._v(" "),void 0!==e.extras?i("div",{staticClass:"extraContainer"},t._l(e.extras,function(e){return i("a",{key:e.path,staticClass:"btn btn-primary",attrs:{href:e.path}},[t._v(t._s(e.name))])})):t._e()])}))]),t._v(" "),i("hr"),t._v(" "),i("h3",[t._v("Camera Network Summarization "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showCamera=!t.showCamera}}},[t._v("View")])]),t._v(" "),i("div",{directives:[{name:"show",rawName:"v-show",value:t.showCamera,expression:"showCamera"}]},[i("img",{staticClass:"research-img",attrs:{src:a("Fzf/"),alt:"Summarization"}}),t._v(" "),i("p",[t._v("\n                 With the recent explosion of big video data, it is becoming increasingly important to automatically extract a brief yet informative summary of these videos in order to enable a more efficient and engaging viewing experience. Video summarization automates this process by providing a succinct representation of a video or a set of videos. The goal of this project is to summarize multiple videos captured in a network of surveillance cameras without requiring any prior knowledge on the field of view of the cameras. We solve the task of summarizing multi-view videos by formulating a sparse representative selection approach over a learned subspace shared by the multiple videos.\n             ")]),t._v(" "),i("h4",[i("span",{staticClass:"publicationHeader"},[t._v("Sample Publications")]),t._v(" "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showCameraPublications=!t.showCameraPublications}}},[t._v("Toggle")])]),t._v(" "),i("ul",{directives:[{name:"show",rawName:"v-show",value:t.showCameraPublications,expression:"showCameraPublications"}],staticClass:"list-group"},t._l(t.cameraPublications,function(e){return i("li",{key:e.name,staticClass:"list-group-item"},[i("a",{attrs:{href:e.link}},[i("h5",[t._v(t._s(e.name))]),t._v(" "),i("p",[i("small",[t._v(t._s(e.note))])])]),t._v(" "),void 0!==e.extras?i("div",{staticClass:"extraContainer"},t._l(e.extras,function(e){return i("a",{key:e.path,staticClass:"btn btn-primary",attrs:{href:e.path}},[t._v(t._s(e.name))])})):t._e()])}))])])])])])},n=[],s={render:i,staticRenderFns:n};e.a=s},gCej:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement;t._self._c;return t._m(0)},n=[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"inherit-height",attrs:{id:"data"}},[a("div",{attrs:{id:"home-body"}},[a("div",{staticClass:"container row"},[a("div",{staticClass:"col-md-12"},[a("h2",{staticClass:"text-center"},[t._v("Open Positions")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("Post-Doctoral Positions:")]),t._v(" There are no vacancies at this time.")]),t._v(" "),a("li",[a("strong",[t._v("Graduate and Undergraduate Positions:")]),t._v(" If you are an existing UCR student and interested in my research areas, please feel free to contact me and we can talk further. If you are interested in graduate school for next Fall, please check the "),a("a",{attrs:{href:"http://www.ee.ucr.edu/prospective-students/"}},[t._v("EE webpage")]),t._v(" or the "),a("a",{attrs:{href:"http://www1.cs.ucr.edu/education/graduate/admissions/"}},[t._v("CS webpage")]),t._v(". If you must contact me, please send a "),a("strong",[t._v("brief")]),t._v(" email and include your resume.")])])])]),t._v(" "),a("br")])])}],s={render:i,staticRenderFns:n};e.a=s},gJEX:function(t,e,a){"use strict";function i(t){a("nb8i")}var n=a("+VW6"),s=a("dmKQ"),o=a("VU/8"),r=i,l=o(n.a,s.a,r,"data-v-7c2a4843",null);e.a=l.exports},gqVA:function(t,e,a){t.exports=a.p+"static/img/afosr.820f17c.png"},hXCR:function(t,e,a){"use strict";e.a={name:"NavAmit",data:function(){return{navItems:[{name:"Home",link:"/"},{name:"Publications",link:"/publications"},{name:"Datasets",link:"/datasets"},{name:"People",link:"/people"}],publications:[{name:"Human Robot Vision Network",link:"/HumanRobots"},{name:"Wide Area Scene Analysis in Vision Network",link:"/WideArea"},{name:"Activity Recognition and Prediction",link:"/ActivityRP"},{name:"Situational Awareness Under Constraint Resources",link:"/SituationalAwareness"},{name:"Face Tracking and Recognition",link:"/FaceTR"},{name:"Biological Image Analysis",link:"/BiologicalIA"},{name:"Multi-terminal Video Compression",link:"/MTVideo"}]}}}},hlG1:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement;t._self._c;return t._m(0)},n=[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("footer",{staticClass:"footer"},[a("div",{staticClass:"container text-center"},[a("div",{staticClass:"row",staticStyle:{"margin-top":"25px"}},[a("div",{staticClass:"col-md-12"},[a("p",{staticClass:"text-muted"},[t._v("\n           Department of Electrical and Computer Engineering"),a("br"),t._v("\n           University of California, Riverside"),a("br"),t._v("\n           Copyright © 2017 Video Computing Group\n         ")])])])])])}],s={render:i,staticRenderFns:n};e.a=s},hrZh:function(t,e,a){"use strict";var i=a("bylR"),n=a("Upxw"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.a=o.exports},"hrz/":function(t,e,a){"use strict";e.a={data:function(){return{showCameraNetwork:!1,showCameraPublications:!1,cameraPublications:[{name:"Unsupervised Adaptive Re-identification in Open World Dynamic Camera Networks",note:"R. Panda, A. H. Bhuiyan, V. Murino and A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2017 (Spotlight).",link:"static/publications/cvpr2017reid.pdf",year:"2017"},{name:"Continuous adaptation of multi-camera person identification models through sparse non-redundant representative selection",note:"A. Das, R. Panda, A. Roy-Chowdhury, Computer Vision and Image Understanding, 2016.",link:"static/publications/CVIU_2016_Abir.pdf",extras:[{name:"Supplemental Material",path:"static/publications/Supplementary_CVIU_2016_Abir.pdf"}],year:"2016"},{name:"Temporal Model Adaptation for Person Re-Identification",note:"N. Martinel, C. Micheloni, A. Roy-Chowdhury, European Conf. on Computer Vision, 2016.",link:"static/publications/niki-eccv16.pdf",year:"2016"},{name:"Tracking multiple interacting targets in a camera network",note:"S. Zhang, Y. Zhu, and A. K. Roy-Chowdhury, Computer Vision and Image Understanding special issue on Image Understanding for Real-world Distributed Video Networks. 2015.",link:"static/publications/Shu CVIU final.pdf",year:"2015"},{name:"Re-Identification in the Function Space of Feature Warps",note:"A. Das, N. Martinel, C. Micheloni, A. Roy-Chowdhury, IEEE Trans. on Pattern Analysis and Machine Intelligence, 2015.",link:"static/publications/PAMI14-Reid.pdf",extras:[{name:"Supplemental Material",path:"static/publications/Supplementary-PAMI2014-Reid.pdf"}],year:"2015"},{name:"A Camera Network Tracking (CamNet) Dataset and Performance Baseline",note:"S. Zhang, E. Staudt, T. Faltemier, A. Roy-Chowdhury, IEEE Winter Conference on Applications of Computer Vision, 2015.",link:"egpaper_final.pdf",extras:[{name:"CamNeT Dataset",path:"static/publications/0B7uDdIqGrZlVfmdlOFZyUEg3RHUxaUdSaGVGTTNDT3R0dUNLSFdCSkZYVWk0dE16TFg4cTA.html"}],year:"2015"},{name:"Consistent Re-identification In A Camera Network",note:"A. Das, A. Chakraborty, A. Roy-Chowdhury, European Conf. on Computer Vision, 2014.",link:"static/publications/eccv2014-2.pdf",extras:[{name:"Supplemental Material",path:"static/publications/NCR_ECCV2014_Supplementary.pdf"},{name:"Code",path:"static/publications/NCR_Code.html"}],year:"2014"},{name:"A Stochastic Graph Evolution Framework for Robust Multi-Target Tracking,",note:"B. Song, T. Jeng, E. Staudt, A. Roy-Chowdhury, European Conference on Computer Vision, 2010.",link:"static/publications/eccv2010.pdf",extras:[],year:"2010"},{name:"Distributed Multi-Target Tracking In A Self-Configuring Camera Network",note:"C. Soto, B. Song, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2009.",link:"static/publications/cvpr09.pdf",extras:[],year:"2009"},{name:"Robust Tracking in A Camera Network: A Multi-Objective Optimization Framework",note:"B. Song and A. Roy-Chowdhury, IEEE Journal on Selected Topics in Signal Processing: Special Issue on Distributed Processing in Vision Networks, August 2008.",link:"static/publications/stamp.html",extras:[],year:"2008"},{name:"Stochastic Adaptive Tracking In A Camera Network",note:"B. Song, A. Roy-Chowdhury, IEEE Intl. Conf. on Computer Vision, 2007.",link:"static/publications/iccv07.pdf",extras:[],year:"2007"}],showDistributed:!1,showDistributedPublications:!1,distributedPublications:[{name:"Distributed Multi-target Tracking and Data Association in Vision Networks",note:"A. T. Kamal, J. H. Bappy, J. A. Farrell, A. Roy-Chowdhury, IEEE Trans. on Pattern Analysis and Machine Intelligence, 2016.",link:"static/publications/MTIC-TPAMI.pdf",extras:[{name:"Code",path:"static/publications/MTIC_Matlab_Code.zip"}],year:"2016"},{name:"Information Weighted Consensus Filters and their Application in Distributed Camera Networks",note:"A. Kamal, J. A. Farrell, A. Roy-Chowdhury, IEEE Trans. on Automatic Control, 2013.",link:"static/publications/ICF_TAC.pdf",extras:[{name:"Code",path:"static/publications/ICF_Matlab_Code.zip"}],year:"2013"},{name:"Information Consensus for Distributed Multi-Target Tracking",note:"A. Kamal, J. A. Farrell, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2013.",link:"static/publications/CVPR2013_MTIC.pdf",extras:[{name:"Supplemental Material",path:"static/publication/CVPR2013_MTIC_Supplementary.pdf"},{name:"Code",path:"static/publication/MTIC_Matlab_Code (1).zip"}],year:"2013"},{name:"Information Weighted Consensus",note:"A. Kamal, J. A. Farrell, A. Roy-Chowdhury, IEEE Controls and Decision Conf., 2012.",link:"static/publications/CDC_2012.pdf",extras:[{name:"Code",path:"static/publications/CameraNetworks.php"}],year:"2012"},{name:"Integrated Sensing and Analysis for Wide Area Scene Understanding",note:"B. Song, C. Ding, A. Kamal, J. Farrell, A. Roy-Chowdhury, Signal Processing Magazine, May 2011.",link:"static/publications/SPM_camnetwork.pdf",extras:[],year:"2011"},{name:"Tracking and Activity Recognition Through Consensus in Distributed Camera Networks",note:"B. Song, A. Kamal, C. Soto, C. Ding, J. Farrell, A. Roy-Chowdhury, IEEE Trans. on Image Processing, 2010.",link:"static/publications/TIP_consensus2010.pdf",extras:[],year:"2010"},{name:"Distributed Multi-Target Tracking In A Self-Configuring Camera Network",note:"C. Soto, B. Song, A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2009.",link:"static/publications/cvpr09.pdf",extras:[],year:"2009"},{name:"Robust Tracking in A Camera Network: A Multi-Objective Optimization Framework",note:"B. Song and A. Roy-Chowdhury, IEEE Journal on Selected Topics in Signal Processing: Special Issue on Distributed Processing in Vision Networks, August 2008.",link:"static/publications/stamp.html",extras:[],year:"2008"},{name:"Stochastic Adaptive Tracking In A Camera Network",note:"B. Song, A. Roy-Chowdhury, IEEE Intl. Conf. on Computer Vision, 2007.",link:"static/publications/iccv07.pdf",extras:[],year:"2007"}],showActiveSensing:!1,showActiveSensingPublications:!1,activeSensingPublications:[{name:"Opportunistic Image Acquisition of Individual and Group Activities in a Distributed Camera Network",note:"C. Ding, J. H. Bappy, J. A. Farrell, A. Roy-Chowdhury, IEEE Transactions on Circuits and Systems for Video Technology, 2016.",link:"static/publications/TCSVT_2016.pdf",year:"2016"},{name:"Distributed Constrained Optimization for Bayesian Opportunistic Visual Sensing",note:"A. Morye, C. Ding, J. A. Farrell, A. Roy-Chowdhury, IEEE Trans. on Control Systems Technology, 2014.",link:"static/publications/akshay_tcst.pdf",extras:[],year:"2014"},{name:"Collaborative Sensing In A Distributed PTZ Camera Network",note:"C. Ding, B. Song, A. Morye, J. A. Farrell, A. Roy-Chowdhury, IEEE Trans. on Image Processing, 2012.",link:"static/publications/tip_cding2012.pdf",extras:[],year:"2012"},{name:"Coordinated Sensing and Tracking for Mobile Camera Platforms",note:"C. Ding, A. Morye, J. A. Farrell, A. Roy-Chowdhury, American Controls Conf., 2012.",link:"static/publications/ACC_2012.pdf",extras:[],year:"2012"}]}}}},iY3p:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"home-body"},[a("div",{staticClass:"row"},[t._m(0),t._v(" "),a("div",{staticClass:"col-md-4"},[a("h3",[t._v("\n           Open Positions\n         ")]),t._v(" "),a("p",[t._v("\n           The Video Computing Group is looking for highly motivated and talented graduate and undergraduate students. If interested, please check  "),a("router-link",{attrs:{to:"positions"}},[t._v("here")]),t._v(" for more details. \n         ")],1)])]),t._v(" "),a("hr")])},n=[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"col-md-8"},[a("h3",[t._v("\n             The Video Computing Group at the University of California, Riverside conducts cutting edge research in computer vision, image processing, statistical learning and signal processing.\n         ")]),t._v(" "),a("p",[t._v("\n             Current projects are related to camera networks, event recognition and prediction, human-machine coordination, resource-constrained visual analysis, active sensing and navigation, and bio-image analysis. The work provides the scientific underpinning behind applications capable of automated/semi-automated analysis of the 3D environment from images/videos, analogous to capabilities of biological visual systems. Members of the group regularly publish in top-tier conferences and journals in computer vision and image processing. Past members work in major research labs and hold faculty positions across the world.\n         ")])])}],s={render:i,staticRenderFns:n};e.a=s},"l+Fy":function(t,e,a){t.exports=a.p+"static/img/mayachitra.d83a5f0.png"},l34S:function(t,e,a){t.exports=a.p+"static/img/graph-structure-activity-forecasting.a80ac48.png"},lERY:function(t,e,a){"use strict";function i(t){a("BMCm")}var n=a("Nf5o"),s=a("+R3I"),o=a("VU/8"),r=i,l=o(n.a,s.a,r,"data-v-2cbcf7cc",null);e.a=l.exports},lGGH:function(t,e,a){t.exports=a.p+"static/img/lockheed.e5a6c9a.png"},lO7g:function(t,e,a){"use strict";var i=a("Fs8J"),n=a("R6fH"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.a=o.exports},lY1t:function(t,e,a){t.exports=a.p+"static/img/amitrc_cover.bc92ac1.jpg"},lh1m:function(t,e,a){t.exports=a.p+"static/img/FACES14.c5db533.png"},ljyK:function(t,e,a){"use strict";e.a={data:function(){return{showAnalysis:!1,showAnalysisPublications:!1,analysisPublications:[{name:"Managing Redundant Content in Bandwidth Constrained Wireless Networks",note:"T. Dao, S. Krishnamurthy, A. Roy-Chowdhury, T. LaPorta, International Conf. on emerging Networking EXperiments and Technologies, 2014.",link:"static/publications/conext_2014.pdf",extras:[],year:"2014"},{name:"Generating Diverse Image Datasets with Limited Labeling",note:"N. C. Mithun, R. Panda, A. Roy-Chowdhury, ACM International Conf. on Multimedia, 2016.",link:"static/publications/ACM_2016.pdf",year:"2016"},{name:"Opportunistic Image Acquisition of Individual and Group Activities in a Distributed Camera Network",note:"C. Ding, J. H. Bappy, J. A. Farrell, A. Roy-Chowdhury, IEEE Transactions on Circuits and Systems for Video Technology, 2016.",link:"static/publications/TCSVT_2016.pdf",year:"2016"},{name:"Multi-View Surveillance Video Summarization via Joint Embedding and Sparse Optimization",note:"R. Panda and A. Roy-Chowdhury, IEEE Trans. on Multimedia, 2017.",link:"static/publications/TMM_Rameswar.pdf",year:"2017"},{name:"Unsupervised Adaptive Re-identification in Open World Dynamic Camera Networks",note:"R. Panda, A. H. Bhuiyan, V. Murino and A. Roy-Chowdhury, IEEE Conf. on Computer Vision and Pattern Recognition, 2017 (Spotlight).",link:"static/publications/cvpr2017reid.pdf",year:"2017"},{name:"Weakly Supervised Summarization of Web Videos",note:"R. Panda, A. Das, Z. Wu, J. Ernst and A. Roy-Chowdhury, International Conference on Computer Vision, 2017.",link:"static/publications/ICCV_Jawad.pdf",year:"2017"}]}}}},mewy:function(t,e,a){t.exports=a.p+"static/img/Figure2.9b20e21.png"},nb8i:function(t,e){},ngh8:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{staticClass:"inherit-height",attrs:{id:"data"}},[i("div",{attrs:{id:"home-body"}},[i("div",{staticClass:"container row"},[i("div",{staticClass:"col-md-12"},[i("h2",{attrs:{id:"name"}},[t._v("Biological Image Analysis")]),t._v(" "),i("img",{staticClass:"research-img",attrs:{src:a("DIZM"),alt:"Graphical Abstract"}}),t._v(" "),i("p",[t._v("\n             Local spatio-temporal co-ordination of cell growth and cell division is important for proper development of organs in both plant and animal systems. The focus of this inter-disciplinary research project is in identification, spatio-temporal modeling and recognition of dynamical patterns inherent in developmental biology through the use of novel computational tools in image analysis, statistical data aggregation, pattern recognition, machine learning and dynamical modeling. This will lead to the development of new methods for addressing some outstanding challenges in this area, i.e., computing cell lineages, identifying long-term patterns in the tracked output and learning functional models of the dynamics of cell growth and division. The computational tool-kit will enable us to gain new insights into the active interplay between cell growth, cell division and changes in gene expression patterns in dynamic developmental fields.This project involves active collaboration with Prof. G. Venugopal Reddy of Plant Biology at UCR.\n           ")]),t._v(" "),t._m(0),t._v(" "),i("h4",[i("span",{staticClass:"publicationHeader"},[t._v("Sample Publications")]),t._v(" "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showBioPublications=!t.showBioPublications}}},[t._v("Toggle")])]),t._v(" "),i("ul",{directives:[{name:"show",rawName:"v-show",value:t.showBioPublications,expression:"showBioPublications"}],staticClass:"list-group"},t._l(t.bioPublications,function(e){return i("li",{key:e.name,staticClass:"list-group-item"},[i("a",{attrs:{href:e.link}},[i("h5",[t._v(t._s(e.name))]),t._v(" "),i("p",[i("small",[t._v(t._s(e.note))])])]),t._v(" "),void 0!==e.extras?i("div",{staticClass:"extraContainer"},t._l(e.extras,function(e){return i("a",{key:e.path,staticClass:"btn btn-primary",attrs:{href:e.path}},[t._v(t._s(e.name))])})):t._e()])}))])])])])},n=[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ul",[a("li",[a("a",{attrs:{href:"../../../static/img/research/Context-CRF-Tracker.zip"}},[t._v("Context-aware spatio-temporal cell tracker")])]),t._v(" "),a("li",[a("a",{attrs:{href:"../../../static/img/research/AQVT.zip"}},[t._v("Matlab code for cell-resolution 3D reconstruction")])]),t._v(" "),a("li",[a("a",{attrs:{href:"../../../static/img/research/celltracking.php"}},[t._v('Matlab code for "local graph" based cell tracking')])])])}],s={render:i,staticRenderFns:n};e.a=s},"oRc/":function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{staticClass:"inherit-height",attrs:{id:"data"}},[i("div",{attrs:{id:"home-body"}},[i("div",{staticClass:"container row"},[i("div",{staticClass:"col-md-12"},[i("h2",{attrs:{id:"name"}},[t._v("Situational Awareness Under Resource Constraints")]),t._v(" "),i("img",{staticClass:"research-img",attrs:{src:a("yoXn"),alt:"Overview"}}),t._v(" "),i("p",[t._v("\n               The goal of this project is to facilitate the timely retrieval of dynamic situational awareness information from field deployed information-rich sensors by an operational center in disaster recovery or search and rescue missions, which are typically characterized by resource-constrained uncertain environments. Towards realizing a networked system that facilitates the retrieval of time-critical, operation-relevant situational awareness this project will address the following (non-exhaustive list) challenges: (a) How do we intelligently activate field sensors and acquire and process data to extract semantically relevant information that is easily interpreted? (b) How do we formulate expressive and effective queries that enable the near-time retrieval of the relevant situational awareness information while adhering to resource constraints? (c) How do we impose a network structure that facilitates cost-effective query propagation and response retrieval? The project encompasses the following three highly inter-related tasks:\n             ")]),t._v(" "),t._m(0),t._v(" "),t._m(1),t._v(" "),t._m(2),t._v(" "),i("hr"),t._v(" "),i("h3",[t._v("Video Analysis under Resource Constraints "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showAnalysis=!t.showAnalysis}}},[t._v("View")])]),t._v(" "),i("div",{directives:[{name:"show",rawName:"v-show",value:t.showAnalysis,expression:"showAnalysis"}]},[i("p",[t._v("\n               Analysis of videos is known to be time-consuming and resource hungry. We are developing methods for scene understanding in video with limited resources. Specifically, we have developed methods for object detection and tracking that are aware of the resource constraints, as well as object and scene categorization methods which are computationally more efficient than many state-of-the-art methods.\n             ")]),t._v(" "),i("h4",[i("span",{staticClass:"publicationHeader"},[t._v("Sample Publications")]),t._v(" "),i("button",{staticClass:"btn btn-primary",on:{click:function(e){t.showAnalysisPublications=!t.showAnalysisPublications}}},[t._v("Toggle")])]),t._v(" "),i("ul",{directives:[{name:"show",rawName:"v-show",value:t.showAnalysisPublications,expression:"showAnalysisPublications"}],staticClass:"list-group"},t._l(t.analysisPublications,function(e){return i("li",{key:e.name,staticClass:"list-group-item"},[i("a",{attrs:{href:e.link}},[i("h5",[t._v(t._s(e.name))]),t._v(" "),i("p",[i("small",[t._v(t._s(e.note))])])]),t._v(" "),void 0!==e.extras?i("div",{staticClass:"extraContainer"},t._l(e.extras,function(e){return i("a",{key:e.path,staticClass:"btn btn-primary",attrs:{href:e.path}},[t._v(t._s(e.name))])})):t._e()])}))])])])])])},n=[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("p",[a("strong",[t._v("Task A")]),t._v(": Resource-Constrained Data Acquisition and Analysis. This task looks at how to reconfigure the network and adapt video analysis in real time to meet different (sometimes conflicting) application requirements, given resource constraints.\n             ")])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("p",[a("strong",[t._v("Task B")]),t._v(": Information Fusion Under Resource Constraints. This task proposes methods to locally process and fuse the content generated, given the query needs and resource constraints. It also considers how to summarize the content received in response to the queries to facilitate further analysis at the operation center.\n             ")])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("p",[a("strong",[t._v("Task C")]),t._v(": Cost-effective Query Formulation and Retrieval. This task will address challenges in query formulation, refinement and retrieval, including (i) prioritizing queries as per importance criteria, (ii) effective query dissemination in the field, and (iii) effective retrieval of the sensed information.\n             ")])}],s={render:i,staticRenderFns:n};e.a=s},pBMn:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("nav",{staticClass:"navbar navbar-default"},[a("div",{staticClass:"container-fluid"},[t._m(0),t._v(" "),a("div",{staticClass:"collapse navbar-collapse",attrs:{id:"bs-example-navbar-collapse-1"}},[a("ul",{staticClass:"nav navbar-nav"},[t._l(t.navItems,function(e){return a("li",{key:e.name},[a("router-link",{attrs:{to:e.link}},[t._v(t._s(e.name))])],1)}),t._v(" "),a("li",{staticClass:"dropdown"},[t._m(1),t._v(" "),a("ul",{staticClass:"dropdown-menu"},[t._l(t.publications,function(e){return a("li",{key:e.name},[a("router-link",{attrs:{to:e.link}},[t._v(t._s(e.name))])],1)}),t._v(" "),t._m(2)],2)])],2)])])])},n=[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"navbar-header"},[a("button",{staticClass:"navbar-toggle collapsed",attrs:{type:"button","data-toggle":"collapse","data-target":"#bs-example-navbar-collapse-1","aria-expanded":"false"}},[a("span",{staticClass:"sr-only"},[t._v("Toggle navigation")]),t._v(" "),a("span",{staticClass:"icon-bar"}),t._v(" "),a("span",{staticClass:"icon-bar"}),t._v(" "),a("span",{staticClass:"icon-bar"})])])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("a",{staticClass:"dropdown-toggle",attrs:{href:"#","data-toggle":"dropdown",role:"button","aria-haspopup":"true","aria-expanded":"false"}},[t._v("Research Projects"),a("span",{staticClass:"caret"})])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("li",[a("a",{attrs:{href:"http://vislab.ucr.edu/RESEARCH/Projects/VideoWeb/index.htm"}},[t._v("VideoWeb: A Video Network Lab")])])}],s={render:i,staticRenderFns:n};e.a=s},pYmz:function(t,e,a){"use strict";Object.defineProperty(e,"__esModule",{value:!0});var i=a("7oBO"),n=a("eJDZ"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.default=o.exports},rKk8:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement;return(t._self._c||e)("div",{class:t.slideClass},[t._t("default")],2)},n=[],s={render:i,staticRenderFns:n};e.a=s},rf4J:function(t,e,a){"use strict";var i=a("8z7O"),n=a("ngh8"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.a=o.exports},seTK:function(t,e,a){t.exports=a.p+"static/img/nga.544bb7f.png"},sgov:function(t,e,a){t.exports=a.p+"static/img/amit.a25239d.jpg"},tPp8:function(t,e,a){"use strict";var i=a("bOdI"),n=a.n(i),s=a("F3EI");a.n(s);e.a={name:"carrousel",data:function(){var t;return{news:[{name:"Drones to Grow Mind of Their Own",link:"https://ucrtoday.ucr.edu/49421",image:"Drone.jpg"},{name:"Diversity-aware Multi-Video Summarization",link:"TIP_Rameswar.pdf",image:"tip.png"},{name:"Multi-View Surveillance Video Summarization via Joint Embedding and Sparse Optimization",link:"TMM_Rameswar.pdf",image:"tmm.png"},{name:"Unsupervised Adaptive Re-identification in Open World Dynamic Camera Networks",link:"cvpr2017reid.pdf",image:"cvpr.png"},{name:"Collaborative Summarization of Topic-Related Videos",link:"cvpr2017summ.pdf",image:"cvpr2017summ.png"},{name:"Non-Uniform Subset Selection for Active Learning in Structured Data",link:"cvpr2017subset.pdf",image:"cvpr2017typical.png"},{name:"The Impact of Typicality for Informative Representative Selection",link:"cvpr2017typicality.pdf",image:"cvpr2017typicality.png"},{name:"Online Adaptation for Joint Scene and Object Classification",link:"eccv2016_jawad.pdf",image:"eccv2016_jawad.png"},{name:"Temporal Model Adaptation for Person Re-Identification",link:"niki-eccv16.pdf",image:"niki-eccv16.png"}],notNextTick:!0,swiperOption:(t={paginationClickable:!0,autoplay:1e3,slidesPerView:4},n()(t,"paginationClickable",!0),n()(t,"spaceBetween",30),n()(t,"grabCursor",!0),t)}}}},udlE:function(t,e,a){"use strict";e.a={data:function(){return{showFace:!1,showFacePublications:!1,facePublications:[{name:"Computerized Face Recognition in Renaissance Portrait Art",note:"R. Srinivasan, C. Rudolph, and A. K. Roy-Chowdhury, Signal Processing Magazine. 2015.",link:"static/publications/spm2015.pdf",extras:[{name:"Supplmental Material",path:"static/publications/SPM2015_SM.pdf"}],year:"2015"},{name:"Recognizing the Royals - Leveraging Computerized Face Recognition for Identifying Subjects in Ancient Artworks",note:"R. Srinivasan, A. Roy-Chowdhury, C. Rudolph, J. Kohl, ACM Intl. Conf. on Multimedia, 2013.",link:"static/publications/acm-mm2013.pdf",extras:[],year:"2013"}]}}}},v2ns:function(t,e){},vaqm:function(t,e,a){"use strict";e.a={name:"HeaderAmit"}},"vt/o":function(t,e,a){"use strict";var i=a("hrz/"),n=a("Ya1q"),s=a("VU/8"),o=s(i.a,n.a,null,null,null);e.a=o.exports},wHfC:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement;t._self._c;return t._m(0)},n=[function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{staticClass:"inherit-height",attrs:{id:"data"}},[i("div",{attrs:{id:"home-body"}},[i("div",{staticClass:"container row"},[i("div",{staticClass:"col-md-12"},[i("h2",{attrs:{id:"name"}},[t._v("Situational Awareness Under Resource Constraints")]),t._v(" "),i("img",{staticClass:"research-img",attrs:{src:a("yoXn"),alt:"Overview"}}),t._v(" "),i("p",[t._v("\n               The goal of this project is to facilitate the timely retrieval of dynamic situational awareness information from field deployed information-rich sensors by an operational center in disaster recovery or search and rescue missions, which are typically characterized by resource-constrained uncertain environments. Towards realizing a networked system that facilitates the retrieval of time-critical, operation-relevant situational awareness this project will address the following (non-exhaustive list) challenges: (a) How do we intelligently activate field sensors and acquire and process data to extract semantically relevant information that is easily interpreted? (b) How do we formulate expressive and effective queries that enable the near-time retrieval of the relevant situational awareness information while adhering to resource constraints? (c) How do we impose a network structure that facilitates cost-effective query propagation and response retrieval? The project encompasses the following three highly inter-related tasks:\n             ")]),t._v(" "),i("p",[i("strong",[t._v("Task A")]),t._v(": Resource-Constrained Data Acquisition and Analysis. This task looks at how to reconfigure the network and adapt video analysis in real time to meet different (sometimes conflicting) application requirements, given resource constraints.\n             ")]),t._v(" "),i("p",[i("strong",[t._v("Task B")]),t._v(": Information Fusion Under Resource Constraints. This task proposes methods to locally process and fuse the content generated, given the query needs and resource constraints. It also considers how to summarize the content received in response to the queries to facilitate further analysis at the operation center.\n             ")]),t._v(" "),i("p",[i("strong",[t._v("Task C")]),t._v(": Cost-effective Query Formulation and Retrieval. This task will address challenges in query formulation, refinement and retrieval, including (i) prioritizing queries as per importance criteria, (ii) effective query dissemination in the field, and (iii) effective retrieval of the sensed information.\n             ")]),t._v(" "),i("hr"),t._v(" "),i("h3",[t._v("Video Analysis under Resource Constraints")]),t._v(" "),i("p",[t._v("\n             Analysis of videos is known to be time-consuming and resource hungry. We are developing methods for scene understanding in video with limited resources. Specifically, we have developed methods for object detection and tracking that are aware of the resource constraints, as well as object and scene categorization methods which are computationally more efficient than many state-of-the-art methods.\n           ")])])])])])}],s={render:i,staticRenderFns:n};e.a=s},"x+8f":function(t,e,a){t.exports=a.p+"static/media/CorridorDemo.bab02c5.mp4"},xJD8:function(t,e,a){"use strict";e.a={name:"app"}},xq7R:function(t,e,a){t.exports=a.p+"static/img/dapra.f5f14b2.png"},yVyP:function(t,e,a){"use strict";var i=function(){var t=this,e=t.$createElement;t._self._c;return t._m(0)},n=[function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{staticClass:"row header-parent"},[i("div",{staticClass:"col-md-3 header-img-container"},[i("img",{staticClass:"header-img",attrs:{src:a("lY1t"),alt:"The Team"}})]),t._v(" "),i("div",{staticClass:"col-md-3 header-img-container"},[i("img",{staticClass:"header-img",attrs:{src:a("cZ8c"),alt:""}})]),t._v(" "),i("div",{staticClass:"col-md-3 header-img-container"},[i("img",{staticClass:"header-img",attrs:{src:a("7A/x"),alt:""}})]),t._v(" "),i("div",{staticClass:"col-md-3 header-img-container"},[i("img",{staticClass:"header-img",attrs:{src:a("Xy5h"),alt:""}})])])}],s={render:i,staticRenderFns:n};e.a=s},yoXn:function(t,e,a){t.exports=a.p+"static/img/Overview.963e0f2.png"},yz8x:function(t,e){}},["NHnr"]);
//# sourceMappingURL=app.fb5b3948648a4f3f2372.js.map